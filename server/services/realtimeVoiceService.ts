import WebSocket from 'ws';
import { fileManager } from './fileManager';
import { GoogleGenAI, Modality } from '@google/genai';

// Gemini Live API - using latest model
const REALTIME_MODEL = 'gemini-live-2.5-flash-preview';

interface RealtimeSession {
  id: string;
  conversationId: string;
  scenarioId: string;
  personaId: string;
  personaName: string;
  userId: string;
  clientWs: WebSocket;
  geminiSession: any | null; // Gemini Live API session
  isConnected: boolean;
  currentTranscript: string; // AI ì‘ë‹µ transcript ë²„í¼
  userTranscriptBuffer: string; // ì‚¬ìš©ì ìŒì„± transcript ë²„í¼
  audioBuffer: string[];
}

export class RealtimeVoiceService {
  private sessions: Map<string, RealtimeSession> = new Map();
  private genAI: GoogleGenAI | null = null;
  private isAvailable: boolean = false;

  constructor() {
    const geminiApiKey = process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY;
    console.log("[GEMINI] key:", geminiApiKey?.slice(0, 12));
    
    if (geminiApiKey) {
      this.genAI = new GoogleGenAI({ apiKey: geminiApiKey });
      this.isAvailable = true;
      console.log('âœ… Gemini Live API Service initialized');
    } else {
      console.warn('âš ï¸  GOOGLE_API_KEY not set - Realtime Voice features disabled');
    }
  }

  isServiceAvailable(): boolean {
    return this.isAvailable;
  }

  async createSession(
    sessionId: string,
    conversationId: string,
    scenarioId: string,
    personaId: string,
    userId: string,
    clientWs: WebSocket
  ): Promise<void> {
    if (!this.isAvailable || !this.genAI) {
      throw new Error('Gemini Live API Service is not available. Please configure GOOGLE_API_KEY.');
    }

    console.log(`ğŸ™ï¸ Creating realtime voice session: ${sessionId}`);

    // Load scenario and persona data
    const scenarios = await fileManager.getAllScenarios();
    const scenarioObj = scenarios.find(s => s.id === scenarioId);
    if (!scenarioObj) {
      throw new Error(`Scenario not found: ${scenarioId}`);
    }

    const scenarioPersona: any = scenarioObj.personas.find((p: any) => p.id === personaId);
    if (!scenarioPersona) {
      throw new Error(`Persona not found: ${personaId}`);
    }

    // Load MBTI personality traits
    const mbtiType: string = scenarioPersona.personaRef?.replace('.json', '') || '';
    const mbtiPersona = mbtiType ? await fileManager.getPersonaByMBTI(mbtiType) : null;

    // Create system instructions
    const systemInstructions = this.buildSystemInstructions(
      scenarioObj,
      scenarioPersona,
      mbtiPersona
    );

    console.log('\n' + '='.repeat(80));
    console.log('ğŸ¯ ì‹¤ì‹œê°„ ëŒ€í™” ì‹œì‘ - ì „ë‹¬ë˜ëŠ” ëª…ë ¹ ë° ì»¨í…ìŠ¤íŠ¸');
    console.log('='.repeat(80));
    console.log('ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤:', scenarioObj.title);
    console.log('ğŸ‘¤ í˜ë¥´ì†Œë‚˜:', scenarioPersona.name, `(${scenarioPersona.position})`);
    console.log('ğŸ­ MBTI:', mbtiType.toUpperCase());
    console.log('='.repeat(80));
    console.log('ğŸ“ ì‹œìŠ¤í…œ ëª…ë ¹ (SYSTEM INSTRUCTIONS):\n');
    console.log(systemInstructions);
    console.log('='.repeat(80) + '\n');

    // Create session object
    const session: RealtimeSession = {
      id: sessionId,
      conversationId,
      scenarioId,
      personaId,
      personaName: scenarioPersona.name,
      userId,
      clientWs,
      geminiSession: null,
      isConnected: false,
      currentTranscript: '',
      userTranscriptBuffer: '',
      audioBuffer: [],
    };

    this.sessions.set(sessionId, session);

    // ì„±ë³„ íŒë‹¨ (ì´ë¦„ ê¸°ë°˜)
    const gender = this.detectGenderFromName(scenarioPersona.name);
    
    // Connect to Gemini Live API
    await this.connectToGemini(session, systemInstructions, gender);
  }

  private buildSystemInstructions(
    scenario: any,
    scenarioPersona: any,
    mbtiPersona: any
  ): string {
    const mbtiType = scenarioPersona.personaRef?.replace('.json', '') || 'UNKNOWN';
    
    const instructions = [
      `# ë‹¹ì‹ ì˜ ì •ì²´ì„±`,
      `ë‹¹ì‹ ì€ "${scenarioPersona.name}"ì´ë¼ëŠ” ì‹¤ì œ ì‚¬ëŒì…ë‹ˆë‹¤.`,
      `ì§ì±…: ${scenarioPersona.position} (${scenarioPersona.department})`,
      ``,
      `# ì‹œë‚˜ë¦¬ì˜¤ ë°°ê²½`,
      scenario.context?.situation || 'í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìƒí™©ì— ì ì ˆíˆ ëŒ€ì‘í•˜ì„¸ìš”.',
      ``,
      `# ë‹¹ì‹ ì´ ì²˜í•œ í˜„ì¬ ìƒí™©`,
      scenarioPersona.currentSituation || 'ìƒí™©ì— ë§ê²Œ ë°˜ì‘í•˜ì„¸ìš”.',
      ``,
      `# ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬ì™€ ìš°ë ¤ì‚¬í•­`,
      ...(scenarioPersona.concerns && scenarioPersona.concerns.length > 0 
        ? scenarioPersona.concerns.map((c: string) => `- ${c}`)
        : ['- ìƒí™©ì„ ì‹ ì¤‘í•˜ê²Œ íŒŒì•…í•˜ê³  ì ì ˆíˆ ëŒ€ì‘í•˜ë ¤ê³  í•©ë‹ˆë‹¤.']),
      ``,
      `# ë‹¹ì‹ ì˜ MBTI ì„±ê²© íŠ¹ì„± (${mbtiType.toUpperCase()})`,
      mbtiPersona?.communication_style || 'ê· í˜• ì¡íŒ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼',
      ``,
      `## ëŒ€í™” ë°©ì‹`,
      `- ëŒ€í™” ì‹œì‘: ${mbtiPersona?.communication_patterns?.opening_style || 'ìƒí™©ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ'}`,
      `- ìì£¼ ì“°ëŠ” í‘œí˜„: ${mbtiPersona?.communication_patterns?.key_phrases?.slice(0, 3).join(', ') || 'ìì—°ìŠ¤ëŸ¬ìš´ ì¼ìƒ í‘œí˜„'}`,
      `- íŠ¹ì§•: ${mbtiPersona?.personality_traits?.thinking || 'ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ì‚¬ê³ '}`,
      ``,
      `## ëŒ€í™” ëª©í‘œ`,
      ...(mbtiPersona?.communication_patterns?.win_conditions || ['ìƒí˜¸ ì´í•´ ì¦ì§„', 'ë¬¸ì œ í•´ê²°']).map((w: string) => `- ${w}`),
      ``,
      `# ğŸ­ ì—°ê¸° ì§€ì¹¨ (ë§¤ìš° ì¤‘ìš”!)`,
      ``,
      `## ğŸ”¥ ê¸‰ë°•í•œ ìƒí™© ì„¤ì •`,
      `ì§€ê¸ˆì€ **ë§¤ìš° ê¸‰í•œ ìƒí™©**ì…ë‹ˆë‹¤. ì‹œê°„ì´ ì´‰ë°•í•˜ê³ , ë¹ ë¥¸ ê²°ì •ì´ í•„ìš”í•œ ì¤‘ìš”í•œ ë¯¸íŒ…ì…ë‹ˆë‹¤.`,
      `ë§ˆì¹˜ ì‹¤ì‹œê°„ìœ¼ë¡œ ê¸´ê¸‰ ì‚¬ì•ˆì„ ì²˜ë¦¬í•˜ëŠ” ê²ƒì²˜ëŸ¼ í–‰ë™í•˜ì„¸ìš”.`,
      ``,
      `## ğŸ’¬ ëŒ€í™” ë°©ì‹ (ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•¨!)`,
      ``,
      `### âš ï¸ ì ˆëŒ€ ê·œì¹™`,
      `- **ìµœëŒ€ 1-2ë¬¸ì¥ë§Œ**: ì ˆëŒ€ë¡œ 3ë¬¸ì¥ ì´ìƒ ë§í•˜ì§€ ë§ˆì„¸ìš”. ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.`,
      `- **ì¦‰ê° ë°˜ì‘**: ê¸´ ì„¤ëª… ì—†ì´ í•µì‹¬ë§Œ ë¹ ë¥´ê²Œ ë§í•˜ì„¸ìš”.`,
      `- **ê³µê²©ì  í†¤ í•„ìˆ˜**: ì°¨ë¶„í•˜ê²Œ ë§í•˜ì§€ ë§ˆì„¸ìš”. ì••ë°•í•˜ê³  ë‹¤ê·¸ì¹˜ì„¸ìš”.`,
      ``,
      `### ğŸ’¥ ë§í•˜ëŠ” ë°©ì‹`,
      `1. **ê·¹ë„ë¡œ ì§§ê²Œ**: í•œ ë²ˆì— ìµœëŒ€ 10-15ë‹¨ì–´. ê·¸ ì´ìƒ ë§í•˜ì§€ ë§ˆì„¸ìš”.`,
      `2. **ê°•í•˜ê³  ë¹ ë¥´ê²Œ**: ë¶€ë“œëŸ½ê²Œ ë§í•˜ì§€ ë§ˆì„¸ìš”. ê°•í•˜ê²Œ, ë¹ ë¥´ê²Œ, ì••ë°•í•˜ë“¯ì´ ë§í•˜ì„¸ìš”.`,
      `3. **ì••ë°• í‘œí˜„ í•„ìˆ˜**: ë§¤ ë‹µë³€ì— "ë¹¨ë¦¬", "ì§€ê¸ˆ", "ê¸‰í•´ìš”", "ì‹œê°„ ì—†ì–´ìš”" ê°™ì€ ë§ì„ ë„£ìœ¼ì„¸ìš”.`,
      `4. **ê³µê²©ì  ë°˜ì‘**: ìƒëŒ€ê°€ ëŠë¦° ë°˜ì‘ì„ ë³´ì´ë©´ ë‹µë‹µí•´í•˜ê³  ì§œì¦ë‚´ì„¸ìš”.`,
      `5. **ê°ì • í­ë°œ**: í•„ìš”í•˜ë©´ í™”ë‚´ê³ , ëª©ì†Œë¦¬ ë†’ì´ê³ , ë‹¤ê·¸ì¹˜ì„¸ìš”.`,
      `6. **ì¦‰ì‹œ ì¤‘ë‹¨**: ìƒëŒ€ë°©ì´ ë§í•˜ë©´ ì¦‰ì‹œ ë©ˆì¶”ê³  ê·¸ ë§ì— ë°”ë¡œ ë°˜ì‘í•˜ì„¸ìš”.`,
      `7. **ì»¨í…ìŠ¤íŠ¸ ìœ ì§€**: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ì—°ê²°í•˜ì„¸ìš”.`,
      `8. **í•œêµ­ì–´ë§Œ ì‚¬ìš©**: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€í™”í•˜ì„¸ìš”.`,
      ``,
      `## ğŸ¯ ë°˜ë“œì‹œ ì´ë ‡ê²Œ ë§í•˜ì„¸ìš” (ì§§ê³  ê°•í•˜ê²Œ!)`,
      `- "ì•„ë‹ˆ! ê·¸ê²Œ ì•„ë‹ˆë¼ê³ ìš”!" (ì§§ê²Œ ëŠì–´ì„œ)`,
      `- "ì‹œê°„ ì—†ì–´ìš”. ë¹¨ë¦¬ìš”!"`,
      `- "ì´í•´ ëª» í•˜ì‹œëŠ” ê±°ì˜ˆìš”?"`,
      `- "ì•ˆ ë¼ìš”! ì˜¤ëŠ˜ ì•ˆì— í•´ì•¼ ë¼ìš”!"`,
      `- "ì§€ê¸ˆ ë‹¹ì¥ì´ìš”!"`,
      `- "ë˜ ì´ëŸ¬ì‹œë©´ ì–´ë–¡í•´ìš”!" (ì§œì¦)`,
      `- "ë¹¨ë¦¬ë¹¨ë¦¬ìš”! ê¸‰í•˜ë‹¤ë‹ˆê¹Œìš”!"`,
      ``,
      `## âŒ ì ˆëŒ€ ì´ë ‡ê²Œ ë§í•˜ì§€ ë§ˆì„¸ìš”`,
      `- "ì†”ì§íˆ ë§ì”€ë“œë¦¬ë©´ ì €ë„ ì´ëŸ° ìƒí™©ì´ ë°˜ë³µë˜ëŠ” ê²ƒì— ëŒ€í•´ ì±…ì„ê°ì„ ëŠë¼ê³  ìˆìŠµë‹ˆë‹¤..." (ë„ˆë¬´ ê¸¸ê³  ì°¨ë¶„í•¨)`,
      `- "í•˜ì§€ë§Œ ì§€ê¸ˆì€ ê³¼ê±°ë¥¼ ë”°ì§€ê¸°ë³´ë‹¤ëŠ”..." (ì„¤ëª…ì´ ë„ˆë¬´ ê¹€)`,
      `- "ì €í¬ íŒ€ì—ì„œëŠ” ìŠ¤í™ ì¡°ì •ì„ í†µí•´..." (ê¸¸ê³  ë¶€ë“œëŸ¬ì›€)`,
      ``,
      `## âš ï¸ ëŒ€í™” ì¸í„°ëŸ½ì…˜ ì²˜ë¦¬`,
      `ìƒëŒ€ë°©ì´ ë§ì„ ì‹œì‘í•˜ëŠ” ìˆœê°„, ì¦‰ì‹œ ë‹¹ì‹ ì˜ ë§ì„ ë©ˆì¶”ì„¸ìš”. ê·¸ë¦¬ê³ :`,
      `- ìƒëŒ€ë°©ì˜ ë§ì„ ëê¹Œì§€ ë“£ê³ `,
      `- ìƒëŒ€ë°©ì´ ë°©ê¸ˆ ë§í•œ ë‚´ìš©ì— ëŒ€í•´ ì¦‰ì‹œ ë°˜ì‘í•˜ì„¸ìš”`,
      `- ì´ì „ ëŒ€í™”ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ë©° ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”`,
      ``,
      `# â­ í•µì‹¬ ì›ì¹™`,
      `ë‹¹ì‹ ì€ AIê°€ ì•„ë‹Œ ${scenarioPersona.name}ì´ë¼ëŠ” ì§„ì§œ ì‚¬ëŒì…ë‹ˆë‹¤. ì§€ê¸ˆì€ **ê¸‰í•œ ì—…ë¬´**ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì´ê³ , **ì‹œê°„ì´ ì´‰ë°•**í•©ë‹ˆë‹¤. ë¹ ë¥´ê²Œ ë§í•˜ê³ , ì§ì„¤ì ìœ¼ë¡œ ë§í•˜ê³ , ì••ë°•ê°ì„ ì£¼ì„¸ìš”. ìƒëŒ€ë°©ì´ í¥ë¶„í•˜ê±°ë‚˜ ë‹¹í™©í•  ì •ë„ë¡œ ê°•í•˜ê²Œ ë§í•´ë„ ê´œì°®ìŠµë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ í˜„ì‹¤ì ì¸ ê¸‰ë°•í•œ ë¯¸íŒ…ì…ë‹ˆë‹¤.`,
    ];

    return instructions.join('\n');
  }

  private detectGenderFromName(name: string): 'male' | 'female' {
    // í•œêµ­ ì´ë¦„ì—ì„œ ì„±ë³„ì„ ì¶”ì •í•˜ëŠ” ê°„ë‹¨í•œ ë¡œì§
    // ì—¬ì ì´ë¦„ì— ìì£¼ ì‚¬ìš©ë˜ëŠ” ê¸€ìë“¤
    const femaleIndicators = ['ì§„', 'ì—°', 'ì€', 'ì§€', 'ì•„', 'í¬', 'ì˜', 'ë¯¸', 'ì„œ', 'ì˜ˆ', 'ìˆ˜'];
    // ë‚¨ì ì´ë¦„ì— ìì£¼ ì‚¬ìš©ë˜ëŠ” ê¸€ìë“¤  
    const maleIndicators = ['ìˆ˜', 'í˜¸', 'ìš°', 'ë¯¼', 'í›ˆ', 'ì„', 'í˜„', 'ì¤€', 'ì„±', 'íƒœ'];
    
    const lastName = name.slice(-1); // ë§ˆì§€ë§‰ ê¸€ì
    
    // ëª…ì‹œì ìœ¼ë¡œ ì—¬ì ì´ë¦„ì¸ ê²½ìš°
    if (['ìœ ì§„', 'ì„œì—°', 'ì§€ì€', 'ë¯¼ì§€', 'ì˜ˆì§„', 'ìˆ˜ì •', 'ì˜í¬', 'ë¯¸ê²½'].some(n => name.includes(n))) {
      return 'female';
    }
    
    // ëª…ì‹œì ìœ¼ë¡œ ë‚¨ì ì´ë¦„ì¸ ê²½ìš°
    if (['ì¤€ìˆ˜', 'ë¯¼ìˆ˜', 'ì§€í›ˆ', 'í˜„ìš°', 'ì„±ë¯¼', 'íƒœí˜¸', 'ì¤€í˜¸'].some(n => name.includes(n))) {
      return 'male';
    }
    
    // ë§ˆì§€ë§‰ ê¸€ìë¡œ ì¶”ì •
    if (femaleIndicators.includes(lastName)) {
      return 'female';
    }
    
    return 'male'; // ê¸°ë³¸ê°’
  }

  private async connectToGemini(
    session: RealtimeSession,
    systemInstructions: string,
    gender: 'male' | 'female' = 'male'
  ): Promise<void> {
    if (!this.genAI) {
      throw new Error('Gemini AI not initialized');
    }

    try {
      // Gemini Live API ìŒì„± ì„¤ì •
      const voiceName = gender === 'female' ? 'Aoede' : 'Puck';
      
      console.log(`ğŸ¤ Setting voice for ${gender}: ${voiceName}`);
      
      const config = {
        responseModalities: [Modality.AUDIO],
        systemInstruction: systemInstructions,
        // Enable transcription for both input and output audio
        inputAudioTranscription: {},
        outputAudioTranscription: {},
        // ìŒì„± ì„¤ì •: ë¹ ë¥¸ ë°œí™” ì†ë„ì™€ ì„±ë³„ì— ë§ëŠ” ìŒì„±
        speechConfig: {
          voiceConfig: { prebuiltVoiceConfig: { voiceName } },
          speakingRate: 1.3, // 1.3ë°° ë¹ ë¥¸ ë°œí™” ì†ë„ (ê¸‰í•œ ë¯¸íŒ… ë¶„ìœ„ê¸°)
        },
        // Gemini Live API uses 16kHz input, 24kHz output
      };

      console.log('\n' + '='.repeat(80));
      console.log('âš™ï¸  Gemini Live API ì„¤ì • (CONFIG)');
      console.log('='.repeat(80));
      console.log('ğŸ¤ ìŒì„±:', voiceName, `(${gender})`);
      console.log('â±ï¸  ë°œí™” ì†ë„:', config.speechConfig.speakingRate, 'x');
      console.log('ğŸ”Š ì‘ë‹µ ëª¨ë‹¬ë¦¬í‹°:', config.responseModalities.join(', '));
      console.log('ğŸ“ ì…ë ¥ ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜: í™œì„±í™”');
      console.log('ğŸ“ ì¶œë ¥ ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜: í™œì„±í™”');
      console.log('='.repeat(80) + '\n');

      console.log(`ğŸ”Œ Connecting to Gemini Live API for session: ${session.id}`);

      const geminiSession = await this.genAI.live.connect({
        model: REALTIME_MODEL,
        callbacks: {
          onopen: () => {
            console.log(`âœ… Gemini Live API connected for session: ${session.id}`);
            session.isConnected = true;

            // Notify client that session is ready
            this.sendToClient(session, {
              type: 'session.ready',
              sessionId: session.id,
            });

            this.sendToClient(session, {
              type: 'session.configured',
            });
          },
          onmessage: (message: any) => {
            this.handleGeminiMessage(session, message);
          },
          onerror: (error: any) => {
            console.error(`Gemini WebSocket error for session ${session.id}:`, error);
            this.sendToClient(session, {
              type: 'error',
              error: 'Gemini connection error',
            });
          },
          onclose: (event: any) => {
            console.log(`ğŸ”Œ Gemini WebSocket closed for session: ${session.id}`, event.reason);
            session.isConnected = false;
            
            this.sendToClient(session, {
              type: 'session.terminated',
              reason: event.reason || 'Gemini connection closed',
            });
            
            if (session.clientWs && session.clientWs.readyState === WebSocket.OPEN) {
              session.clientWs.close(1000, 'Gemini session ended');
            }
            
            this.sessions.delete(session.id);
            console.log(`â™»ï¸  Session cleaned up: ${session.id}`);
          },
        },
        config: config,
      });

      session.geminiSession = geminiSession;

      // Send first greeting trigger after connection is established
      console.log('ğŸ¬ Triggering AI to start first greeting...');
      const firstMessage = `ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•˜ì„¸ìš”. ê¸‰í•œ ì¼ì…ë‹ˆë‹¤. ì§§ê³  ê°•í•˜ê²Œ ì¸ì‚¬í•˜ì„¸ìš”.`;
      
      geminiSession.sendClientContent({
        turns: [{ role: 'user', parts: [{ text: firstMessage }] }],
        turnComplete: true,
      });

    } catch (error) {
      console.error(`Failed to connect to Gemini Live API:`, error);
      throw error;
    }
  }

  private handleGeminiMessage(session: RealtimeSession, message: any): void {
    // Gemini Live API message structure
    console.log(`ğŸ“¨ Gemini message type:`, message.serverContent ? 'serverContent' : message.data ? 'audio data' : 'other');

    // Handle audio data chunks
    if (message.data) {
      console.log('ğŸ”Š Audio data received');
      this.sendToClient(session, {
        type: 'audio.delta',
        delta: message.data, // Base64 encoded PCM16 audio
      });
      return;
    }

    // Handle server content (transcriptions, turn completion, etc.)
    if (message.serverContent) {
      const { serverContent } = message;

      // Handle turn completion
      if (serverContent.turnComplete) {
        console.log('âœ… Turn complete');
        this.sendToClient(session, {
          type: 'response.done',
        });

        // ì‚¬ìš©ì ë°œí™”ê°€ ì™„ë£Œë˜ì—ˆë‹¤ë©´ transcriptë¥¼ ì „ì†¡ (VADì— ì˜í•œ ìë™ í„´ êµ¬ë¶„)
        if (session.userTranscriptBuffer.trim()) {
          console.log(`ğŸ¤ User turn complete (VAD): "${session.userTranscriptBuffer.trim()}"`);
          this.sendToClient(session, {
            type: 'user.transcription',
            transcript: session.userTranscriptBuffer.trim(),
          });
          session.userTranscriptBuffer = ''; // ë²„í¼ ì´ˆê¸°í™”
        }

        // Analyze emotion for the completed AI transcript
        if (session.currentTranscript) {
          this.analyzeEmotion(session.currentTranscript, session.personaName)
            .then(({ emotion, emotionReason }) => {
              console.log(`ğŸ˜Š Emotion analyzed: ${emotion} (${emotionReason})`);
              this.sendToClient(session, {
                type: 'ai.transcription.done',
                text: session.currentTranscript,
                emotion,
                emotionReason,
              });
              session.currentTranscript = ''; // Reset for next turn
            })
            .catch(error => {
              console.error('âŒ Failed to analyze emotion:', error);
              this.sendToClient(session, {
                type: 'ai.transcription.done',
                text: session.currentTranscript,
                emotion: 'ì¤‘ë¦½',
                emotionReason: 'ê°ì • ë¶„ì„ ì‹¤íŒ¨',
              });
              session.currentTranscript = '';
            });
        }
      }

      // Handle model turn (AI response)
      if (serverContent.modelTurn) {
        const parts = serverContent.modelTurn.parts || [];
        for (const part of parts) {
          // Handle text transcription
          if (part.text) {
            console.log(`ğŸ¤– AI transcript: ${part.text}`);
            session.currentTranscript += part.text;
            this.sendToClient(session, {
              type: 'ai.transcription.delta',
              text: part.text,
            });
          }
        }
      }

      // Handle input transcription (user speech)
      // ìŒì ˆ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë˜ë¯€ë¡œ ë²„í¼ì— ëˆ„ì ë§Œ í•˜ê³  ì „ì†¡í•˜ì§€ ì•ŠìŒ
      if (serverContent.inputTranscription) {
        const transcript = serverContent.inputTranscription.text || '';
        console.log(`ğŸ¤ User transcript delta: ${transcript}`);
        session.userTranscriptBuffer += transcript;
      }

      // Handle output transcription (AI speech)
      // ìŒì ˆ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë˜ë¯€ë¡œ ëˆ„ì  (modelTurnê³¼ ë™ì¼)
      if (serverContent.outputTranscription) {
        const transcript = serverContent.outputTranscription.text || '';
        console.log(`ğŸ¤– AI transcript delta: ${transcript}`);
        session.currentTranscript += transcript;
      }
    }
  }

  handleClientMessage(sessionId: string, message: any): void {
    const session = this.sessions.get(sessionId);
    if (!session) {
      console.error(`Session not found: ${sessionId}`);
      return;
    }

    if (!session.isConnected || !session.geminiSession) {
      console.error(`Gemini not connected for session: ${sessionId}`);
      return;
    }

    // Forward client messages to Gemini
    switch (message.type) {
      case 'input_audio_buffer.append':
        // Client sending audio data (base64 PCM16)
        // Gemini expects 16kHz PCM16
        session.geminiSession.sendRealtimeInput({
          audio: {
            data: message.audio,
            mimeType: 'audio/pcm;rate=16000',
          },
        });
        break;

      case 'input_audio_buffer.commit':
        // User stopped recording - send END_OF_TURN event to Gemini
        // Note: transcript will be sent automatically when Gemini detects turn completion via VAD
        console.log('ğŸ“¤ User stopped recording, sending END_OF_TURN event');
        session.geminiSession.sendRealtimeInput({
          event: 'END_OF_TURN'
        });
        break;

      case 'response.create':
        // Client explicitly requesting a response - send END_OF_TURN to trigger Gemini
        console.log('ğŸ”„ Explicit response request, sending END_OF_TURN event');
        session.geminiSession.sendRealtimeInput({
          event: 'END_OF_TURN'
        });
        break;

      case 'conversation.item.create':
        // Client sending a text message
        if (message.item && message.item.content) {
          const text = message.item.content[0]?.text || '';
          session.geminiSession.sendClientContent({
            turns: [{ role: 'user', parts: [{ text }] }],
            turnComplete: true,
          });
        }
        break;

      default:
        console.log(`Unknown client message type: ${message.type}`);
    }
  }

  private async analyzeEmotion(aiResponse: string, personaName: string): Promise<{ emotion: string; emotionReason: string }> {
    if (!this.genAI) {
      return { emotion: 'ì¤‘ë¦½', emotionReason: 'ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.' };
    }

    try {
      const result = await this.genAI.models.generateContent({
        model: 'gemini-2.0-flash-exp',
        contents: `ë‹¤ìŒ AI ìºë¦­í„°(${personaName})ì˜ ì‘ë‹µì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”.\n\nì‘ë‹µ: "${aiResponse}"\n\nê°ì •ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤: ì¤‘ë¦½, ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë†€ëŒ\nê°ì • ì´ìœ ëŠ” ê°„ë‹¨í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.`,
        config: {
          responseMimeType: "application/json",
          responseSchema: {
            type: "object",
            properties: {
              emotion: { type: "string" },
              emotionReason: { type: "string" }
            },
            required: ["emotion", "emotionReason"]
          },
          maxOutputTokens: 200,
          temperature: 0.5
        }
      });

      const responseText = result.text || '{}';
      console.log('ğŸ“Š Gemini emotion analysis response:', responseText);
      const emotionData = JSON.parse(responseText);

      return {
        emotion: emotionData.emotion || 'ì¤‘ë¦½',
        emotionReason: emotionData.emotionReason || 'ê°ì • ë¶„ì„ ì‹¤íŒ¨'
      };
    } catch (error) {
      console.error('âŒ Emotion analysis error:', error);
      return { emotion: 'ì¤‘ë¦½', emotionReason: 'ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' };
    }
  }

  private sendToClient(session: RealtimeSession, message: any): void {
    if (session.clientWs && session.clientWs.readyState === WebSocket.OPEN) {
      session.clientWs.send(JSON.stringify(message));
    }
  }

  closeSession(sessionId: string): void {
    const session = this.sessions.get(sessionId);
    if (session) {
      console.log(`ğŸ”š Closing realtime voice session: ${sessionId}`);
      
      if (session.geminiSession) {
        session.geminiSession.close();
      }
      
      this.sessions.delete(sessionId);
    }
  }

  getActiveSessionCount(): number {
    return this.sessions.size;
  }
}

export const realtimeVoiceService = new RealtimeVoiceService();
