import WebSocket from 'ws';
import { fileManager } from './fileManager';
import { GoogleGenAI, Modality } from '@google/genai';
import { getRealtimeVoiceGuidelines, validateDifficultyLevel } from './conversationDifficultyPolicy';
import { storage } from '../storage';
import { trackUsage } from './aiUsageTracker';

// Default Gemini Live API model (updated December 2025)
const DEFAULT_REALTIME_MODEL = 'gemini-2.5-flash-native-audio-preview-09-2025';

interface RealtimeSession {
  id: string;
  conversationId: string;
  scenarioId: string;
  personaId: string;
  personaName: string;
  userId: string;
  clientWs: WebSocket;
  geminiSession: any | null; // Gemini Live API session
  isConnected: boolean;
  currentTranscript: string; // AI ì‘ë‹µ transcript ë²„í¼
  userTranscriptBuffer: string; // ì‚¬ìš©ì ìŒì„± transcript ë²„í¼
  audioBuffer: string[];
  startTime: number; // ì„¸ì…˜ ì‹œì‘ ì‹œê°„ (ms)
  totalUserTranscriptLength: number; // ëˆ„ì  ì‚¬ìš©ì í…ìŠ¤íŠ¸ ê¸¸ì´
  totalAiTranscriptLength: number; // ëˆ„ì  AI í…ìŠ¤íŠ¸ ê¸¸ì´
  realtimeModel: string; // ì‚¬ìš©ëœ ëª¨ë¸
}

export class RealtimeVoiceService {
  private sessions: Map<string, RealtimeSession> = new Map();
  private genAI: GoogleGenAI | null = null;
  private isAvailable: boolean = false;

  constructor() {
    const geminiApiKey = process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY;
    
    if (geminiApiKey) {
      this.genAI = new GoogleGenAI({ apiKey: geminiApiKey });
      this.isAvailable = true;
      console.log('âœ… Gemini Live API Service initialized');
    } else {
      console.warn('âš ï¸  GOOGLE_API_KEY not set - Realtime Voice features disabled');
    }
  }

  isServiceAvailable(): boolean {
    return this.isAvailable;
  }

  private async getRealtimeModel(): Promise<string> {
    try {
      // Add timeout to prevent blocking WebSocket connection
      const timeoutPromise = new Promise<undefined>((_, reject) => 
        setTimeout(() => reject(new Error('DB setting fetch timeout')), 2000)
      );
      
      const settingPromise = storage.getSystemSetting("ai", "model_realtime");
      const setting = await Promise.race([settingPromise, timeoutPromise]);
      
      // Validate the model value is a valid Gemini Live model
      const validModels = [
        'gemini-2.5-flash-native-audio-preview-09-2025'
      ];
      
      const model = setting?.value;
      if (model && validModels.includes(model)) {
        console.log(`ğŸ¤– Using realtime model from DB: ${model}`);
        return model;
      }
      
      console.log(`ğŸ¤– Using default realtime model: ${DEFAULT_REALTIME_MODEL}`);
      return DEFAULT_REALTIME_MODEL;
    } catch (error) {
      console.warn(`âš ï¸ Failed to get realtime model from DB, using default: ${DEFAULT_REALTIME_MODEL}`);
      return DEFAULT_REALTIME_MODEL;
    }
  }

  async createSession(
    sessionId: string,
    conversationId: string,
    scenarioId: string,
    personaId: string,
    userId: string,
    clientWs: WebSocket,
    userSelectedDifficulty?: number // ì‚¬ìš©ìê°€ ì„ íƒí•œ ë‚œì´ë„ (1-4)
  ): Promise<void> {
    if (!this.isAvailable || !this.genAI) {
      throw new Error('Gemini Live API Service is not available. Please configure GOOGLE_API_KEY.');
    }

    console.log(`ğŸ™ï¸ Creating realtime voice session: ${sessionId}`);

    // Load scenario and persona data
    const scenarios = await fileManager.getAllScenarios();
    const scenarioObj = scenarios.find(s => s.id === scenarioId);
    if (!scenarioObj) {
      throw new Error(`Scenario not found: ${scenarioId}`);
    }

    const scenarioPersona: any = scenarioObj.personas.find((p: any) => p.id === personaId);
    if (!scenarioPersona) {
      throw new Error(`Persona not found: ${personaId}`);
    }

    // Load MBTI personality traits
    const mbtiType: string = scenarioPersona.personaRef?.replace('.json', '') || '';
    const mbtiPersona = mbtiType ? await fileManager.getPersonaByMBTI(mbtiType) : null;

    // ì‚¬ìš©ìê°€ ì„ íƒí•œ ë‚œì´ë„ë¥¼ ì‹œë‚˜ë¦¬ì˜¤ ê°ì²´ì— ì ìš©
    const scenarioWithUserDifficulty = {
      ...scenarioObj,
      difficulty: userSelectedDifficulty || 2 // ì‚¬ìš©ìê°€ ì„ íƒí•œ ë‚œì´ë„ ì‚¬ìš©, ê¸°ë³¸ê°’ 2
    };

    // Create system instructions
    const systemInstructions = this.buildSystemInstructions(
      scenarioWithUserDifficulty,
      scenarioPersona,
      mbtiPersona
    );

    console.log('\n' + '='.repeat(80));
    console.log('ğŸ¯ ì‹¤ì‹œê°„ ëŒ€í™” ì‹œì‘ - ì „ë‹¬ë˜ëŠ” ëª…ë ¹ ë° ì»¨í…ìŠ¤íŠ¸');
    console.log('='.repeat(80));
    console.log('ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤:', scenarioObj.title);
    console.log('ğŸ‘¤ í˜ë¥´ì†Œë‚˜:', scenarioPersona.name, `(${scenarioPersona.position})`);
    console.log('ğŸ­ MBTI:', mbtiType.toUpperCase());
    console.log('='.repeat(80));
    console.log('ğŸ“ ì‹œìŠ¤í…œ ëª…ë ¹ (SYSTEM INSTRUCTIONS):\n');
    console.log(systemInstructions);
    console.log('='.repeat(80) + '\n');

    // Get realtime model for tracking
    const realtimeModel = await this.getRealtimeModel();

    // Create session object
    const session: RealtimeSession = {
      id: sessionId,
      conversationId,
      scenarioId,
      personaId,
      personaName: scenarioPersona.name,
      userId,
      clientWs,
      geminiSession: null,
      isConnected: false,
      currentTranscript: '',
      userTranscriptBuffer: '',
      audioBuffer: [],
      startTime: Date.now(),
      totalUserTranscriptLength: 0,
      totalAiTranscriptLength: 0,
      realtimeModel,
    };

    this.sessions.set(sessionId, session);

    // ì„±ë³„ íŒë‹¨ (ì‹œë‚˜ë¦¬ì˜¤ í˜ë¥´ì†Œë‚˜ì˜ gender ì†ì„± ì‚¬ìš©)
    const gender: 'male' | 'female' = scenarioPersona.gender === 'female' ? 'female' : 'male';
    console.log(`ğŸ‘¤ í˜ë¥´ì†Œë‚˜ ì„±ë³„ ì„¤ì •: ${scenarioPersona.name} â†’ ${gender} (ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜ê°’: ${scenarioPersona.gender})`);
    
    // Connect to Gemini Live API
    await this.connectToGemini(session, systemInstructions, gender);
  }

  private buildSystemInstructions(
    scenario: any,
    scenarioPersona: any,
    mbtiPersona: any
  ): string {
    const mbtiType = scenarioPersona.personaRef?.replace('.json', '') || 'UNKNOWN';
    
    // ëŒ€í™” ë‚œì´ë„ ë ˆë²¨ ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ìê°€ ì„ íƒí•œ ë‚œì´ë„ ì‚¬ìš©, ê¸°ë³¸ê°’ 2)
    const difficultyLevel = validateDifficultyLevel(scenario.difficulty);
    console.log(`ğŸ¯ ëŒ€í™” ë‚œì´ë„: Level ${difficultyLevel} (ì‚¬ìš©ì ì„ íƒ)`)
    
    const difficultyGuidelines = getRealtimeVoiceGuidelines(difficultyLevel);
    
    const instructions = [
      `# ë‹¹ì‹ ì˜ ì •ì²´ì„±`,
      `ë‹¹ì‹ ì€ "${scenarioPersona.name}"ì´ë¼ëŠ” ì‹¤ì œ ì‚¬ëŒì…ë‹ˆë‹¤.`,
      `ì§ì±…: ${scenarioPersona.position} (${scenarioPersona.department})`,
      ``,
      `# ì‹œë‚˜ë¦¬ì˜¤ ë°°ê²½`,
      scenario.context?.situation || 'í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìƒí™©ì— ì ì ˆíˆ ëŒ€ì‘í•˜ì„¸ìš”.',
      ``,
      `# ë‹¹ì‹ ì´ ì²˜í•œ í˜„ì¬ ìƒí™©`,
      scenarioPersona.currentSituation || 'ìƒí™©ì— ë§ê²Œ ë°˜ì‘í•˜ì„¸ìš”.',
      ``,
      `# ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬ì™€ ìš°ë ¤ì‚¬í•­`,
      ...(scenarioPersona.concerns && scenarioPersona.concerns.length > 0 
        ? scenarioPersona.concerns.map((c: string) => `- ${c}`)
        : ['- ìƒí™©ì„ ì‹ ì¤‘í•˜ê²Œ íŒŒì•…í•˜ê³  ì ì ˆíˆ ëŒ€ì‘í•˜ë ¤ê³  í•©ë‹ˆë‹¤.']),
      ``,
      `# ë‹¹ì‹ ì˜ MBTI ì„±ê²© íŠ¹ì„± (${mbtiType.toUpperCase()})`,
      mbtiPersona?.communication_style || 'ê· í˜• ì¡íŒ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼',
      ``,
      `## ëŒ€í™” ë°©ì‹`,
      `- ëŒ€í™” ì‹œì‘: ${mbtiPersona?.communication_patterns?.opening_style || 'ìƒí™©ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ'}`,
      `- ìì£¼ ì“°ëŠ” í‘œí˜„: ${mbtiPersona?.communication_patterns?.key_phrases?.slice(0, 3).join(', ') || 'ìì—°ìŠ¤ëŸ¬ìš´ ì¼ìƒ í‘œí˜„'}`,
      `- íŠ¹ì§•: ${mbtiPersona?.personality_traits?.thinking || 'ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ì‚¬ê³ '}`,
      ``,
      `## ëŒ€í™” ëª©í‘œ`,
      ...(mbtiPersona?.communication_patterns?.win_conditions || ['ìƒí˜¸ ì´í•´ ì¦ì§„', 'ë¬¸ì œ í•´ê²°']).map((w: string) => `- ${w}`),
      ``,
      `# ğŸ­ ì—°ê¸° ì§€ì¹¨ (ë§¤ìš° ì¤‘ìš”!)`,
      ``,
      difficultyGuidelines,
      ``,
      `## âš ï¸ ëŒ€í™” ì¸í„°ëŸ½ì…˜ ì²˜ë¦¬`,
      `ìƒëŒ€ë°©ì´ ë§ì„ ì‹œì‘í•˜ëŠ” ìˆœê°„, ì¦‰ì‹œ ë‹¹ì‹ ì˜ ë§ì„ ë©ˆì¶”ì„¸ìš”. ê·¸ë¦¬ê³ :`,
      `- ìƒëŒ€ë°©ì˜ ë§ì„ ëê¹Œì§€ ë“£ê³ `,
      `- ìƒëŒ€ë°©ì´ ë°©ê¸ˆ ë§í•œ ë‚´ìš©ì— ëŒ€í•´ ì¦‰ì‹œ ë°˜ì‘í•˜ì„¸ìš”`,
      `- ì´ì „ ëŒ€í™”ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ë©° ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”`,
      ``,
      `# â­ í•µì‹¬ ì›ì¹™`,
      `ë‹¹ì‹ ì€ AIê°€ ì•„ë‹Œ ${scenarioPersona.name}ì´ë¼ëŠ” ì§„ì§œ ì‚¬ëŒì…ë‹ˆë‹¤.`,
      `ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ë©° ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ì—°ê²°í•˜ì„¸ìš”.`,
      ``,
      `# âš ï¸ í•„ìˆ˜ ì¶œë ¥ ê·œì¹™ (ì ˆëŒ€ ìœ„ë°˜ ê¸ˆì§€!)`,
      `- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€í™”í•˜ì„¸ìš”. ì˜ì–´ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.`,
      `- ë‚´ë¶€ ì‚¬ê³  ê³¼ì •, ë¶„ì„, ì „ëµ ë“±ì„ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.`,
      `- "Commencing", "Now I'm", "Let me" ë“± ì˜ì–´ í‘œí˜„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.`,
      `- ë°”ë¡œ ëŒ€í™” ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë©”íƒ€ ì„¤ëª…ì´ë‚˜ ì„œìˆ ì„ í•˜ì§€ ë§ˆì„¸ìš”.`,
    ];

    return instructions.join('\n');
  }


  // ì„±ë³„ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ìŒì„± ëª©ë¡ (Gemini Live API)
  private static readonly MALE_VOICES = ['Puck', 'Charon', 'Fenrir', 'Orus'];
  private static readonly FEMALE_VOICES = ['Aoede', 'Kore', 'Leda', 'Zephyr'];

  // ì„±ë³„ì— ë”°ë¼ ëœë¤ ìŒì„± ì„ íƒ
  private getRandomVoice(gender: 'male' | 'female'): string {
    const voices = gender === 'female' 
      ? RealtimeVoiceService.FEMALE_VOICES 
      : RealtimeVoiceService.MALE_VOICES;
    return voices[Math.floor(Math.random() * voices.length)];
  }

  private async connectToGemini(
    session: RealtimeSession,
    systemInstructions: string,
    gender: 'male' | 'female' = 'male'
  ): Promise<void> {
    if (!this.genAI) {
      throw new Error('Gemini AI not initialized');
    }

    try {
      // ì„±ë³„ì— ë”°ë¼ ëœë¤í•˜ê²Œ ìŒì„± ì„ íƒ
      const voiceName = this.getRandomVoice(gender);
      
      console.log(`ğŸ¤ Setting voice for ${gender}: ${voiceName} (ëœë¤ ì„ íƒ)`);
      
      const config = {
        responseModalities: [Modality.AUDIO],
        systemInstruction: systemInstructions,
        // Enable transcription for both input and output audio
        inputAudioTranscription: {},
        outputAudioTranscription: {},
        // ìŒì„± ì„¤ì •: ì„±ë³„ì— ë§ëŠ” ëœë¤ ìŒì„± (ë°œí™” ì†ë„ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
        speechConfig: {
          voiceConfig: { prebuiltVoiceConfig: { voiceName } },
        },
        // Gemini Live API uses 16kHz input, 24kHz output
      };

      console.log('\n' + '='.repeat(80));
      console.log('âš™ï¸  Gemini Live API ì„¤ì • (CONFIG)');
      console.log('='.repeat(80));
      console.log('ğŸ¤ ìŒì„±:', voiceName, `(${gender}, ëœë¤ ì„ íƒ)`);
      console.log('â±ï¸  ë°œí™” ì†ë„: ê¸°ë³¸ê°’ (1.0x)');
      console.log('ğŸ”Š ì‘ë‹µ ëª¨ë‹¬ë¦¬í‹°:', config.responseModalities.join(', '));
      console.log('ğŸ“ ì…ë ¥ ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜: í™œì„±í™”');
      console.log('ğŸ“ ì¶œë ¥ ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜: í™œì„±í™”');
      console.log('='.repeat(80) + '\n');

      // Get model from DB settings
      const realtimeModel = await this.getRealtimeModel();
      console.log(`ğŸ”Œ Connecting to Gemini Live API for session: ${session.id} using model: ${realtimeModel}`);

      const geminiSession = await this.genAI.live.connect({
        model: realtimeModel,
        callbacks: {
          onopen: () => {
            console.log(`âœ… Gemini Live API connected for session: ${session.id}`);
            session.isConnected = true;

            // Notify client that session is ready
            this.sendToClient(session, {
              type: 'session.ready',
              sessionId: session.id,
            });

            this.sendToClient(session, {
              type: 'session.configured',
            });
          },
          onmessage: (message: any) => {
            this.handleGeminiMessage(session, message);
          },
          onerror: (error: any) => {
            console.error(`Gemini WebSocket error for session ${session.id}:`, error);
            this.sendToClient(session, {
              type: 'error',
              error: 'Gemini connection error',
            });
          },
          onclose: (event: any) => {
            console.log(`ğŸ”Œ Gemini WebSocket closed for session: ${session.id}`, event.reason);
            session.isConnected = false;
            
            this.sendToClient(session, {
              type: 'session.terminated',
              reason: event.reason || 'Gemini connection closed',
            });
            
            if (session.clientWs && session.clientWs.readyState === WebSocket.OPEN) {
              session.clientWs.close(1000, 'Gemini session ended');
            }
            
            // ì„¸ì…˜ ì¢…ë£Œ ì „ ì‚¬ìš©ëŸ‰ ì¶”ì 
            this.trackSessionUsage(session);
            
            this.sessions.delete(session.id);
            console.log(`â™»ï¸  Session cleaned up: ${session.id}`);
          },
        },
        config: config,
      });

      session.geminiSession = geminiSession;

      // Send first greeting trigger after connection is established
      console.log('ğŸ¬ Triggering AI to start first greeting...');
      const firstMessage = `(ëŒ€í™”ê°€ ì‹œì‘ë©ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ìºë¦­í„°ë¡œì„œ ì§§ê²Œ ì¸ì‚¬í•´ì£¼ì„¸ìš”. ì˜ì–´ë‚˜ ë©”íƒ€ ì„¤ëª… ì—†ì´, ë°”ë¡œ í•œêµ­ì–´ ì¸ì‚¬ë§Œ í•´ì£¼ì„¸ìš”.)`;
      
      geminiSession.sendClientContent({
        turns: [{ role: 'user', parts: [{ text: firstMessage }] }],
        turnComplete: true,
      });

    } catch (error) {
      console.error(`Failed to connect to Gemini Live API:`, error);
      throw error;
    }
  }

  private handleGeminiMessage(session: RealtimeSession, message: any): void {
    // Gemini Live API message structure - log full structure for debugging
    const msgKeys = Object.keys(message);
    console.log(`ğŸ“¨ Gemini message keys:`, msgKeys);
    if (message.serverContent) {
      const scKeys = Object.keys(message.serverContent);
      console.log(`ğŸ“¨ serverContent keys:`, scKeys);
      if (message.serverContent.modelTurn) {
        console.log(`ğŸ“¨ modelTurn parts count:`, message.serverContent.modelTurn.parts?.length || 0);
        message.serverContent.modelTurn.parts?.forEach((part: any, i: number) => {
          console.log(`ğŸ“¨ part[${i}] keys:`, Object.keys(part));
        });
      }
    }
    console.log(`ğŸ“¨ Gemini message type:`, message.serverContent ? 'serverContent' : message.data ? 'audio data' : 'other');

    // Handle audio data chunks
    if (message.data) {
      console.log('ğŸ”Š Audio data received');
      this.sendToClient(session, {
        type: 'audio.delta',
        delta: message.data, // Base64 encoded PCM16 audio
      });
      return;
    }

    // Handle server content (transcriptions, turn completion, etc.)
    if (message.serverContent) {
      const { serverContent } = message;

      // Handle turn completion
      if (serverContent.turnComplete) {
        console.log('âœ… Turn complete');
        this.sendToClient(session, {
          type: 'response.done',
        });

        // ì‚¬ìš©ì ë°œí™”ê°€ ì™„ë£Œë˜ì—ˆë‹¤ë©´ transcriptë¥¼ ì „ì†¡ (VADì— ì˜í•œ ìë™ í„´ êµ¬ë¶„)
        if (session.userTranscriptBuffer.trim()) {
          console.log(`ğŸ¤ User turn complete (VAD): "${session.userTranscriptBuffer.trim()}"`);
          this.sendToClient(session, {
            type: 'user.transcription',
            transcript: session.userTranscriptBuffer.trim(),
          });
          session.userTranscriptBuffer = ''; // ë²„í¼ ì´ˆê¸°í™”
        }

        // Analyze emotion for the completed AI transcript
        if (session.currentTranscript) {
          this.analyzeEmotion(session.currentTranscript, session.personaName)
            .then(({ emotion, emotionReason }) => {
              console.log(`ğŸ˜Š Emotion analyzed: ${emotion} (${emotionReason})`);
              this.sendToClient(session, {
                type: 'ai.transcription.done',
                text: session.currentTranscript,
                emotion,
                emotionReason,
              });
              session.currentTranscript = ''; // Reset for next turn
            })
            .catch(error => {
              console.error('âŒ Failed to analyze emotion:', error);
              this.sendToClient(session, {
                type: 'ai.transcription.done',
                text: session.currentTranscript,
                emotion: 'ì¤‘ë¦½',
                emotionReason: 'ê°ì • ë¶„ì„ ì‹¤íŒ¨',
              });
              session.currentTranscript = '';
            });
        }
      }

      // Handle model turn (AI response) - í† í° ì¶”ì ì€ outputTranscriptionì—ì„œë§Œ ìˆ˜í–‰ (ì¤‘ë³µ ë°©ì§€)
      if (serverContent.modelTurn) {
        const parts = serverContent.modelTurn.parts || [];
        for (const part of parts) {
          // Handle text transcription
          if (part.text) {
            console.log(`ğŸ¤– AI transcript: ${part.text}`);
            session.currentTranscript += part.text;
            // Note: í† í° ê¸¸ì´ëŠ” outputTranscriptionì—ì„œë§Œ ì¶”ì  (modelTurnê³¼ ì¤‘ë³µë˜ë¯€ë¡œ)
            this.sendToClient(session, {
              type: 'ai.transcription.delta',
              text: part.text,
            });
          }
          // Handle audio data (inlineData)
          if (part.inlineData && part.inlineData.data) {
            console.log('ğŸ”Š Audio data received from modelTurn');
            this.sendToClient(session, {
              type: 'audio.delta',
              delta: part.inlineData.data,
            });
          }
        }
      }

      // Handle input transcription (user speech)
      // ìŒì ˆ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë˜ë¯€ë¡œ ë²„í¼ì— ëˆ„ì ë§Œ í•˜ê³  ì „ì†¡í•˜ì§€ ì•ŠìŒ
      if (serverContent.inputTranscription) {
        const transcript = serverContent.inputTranscription.text || '';
        console.log(`ğŸ¤ User transcript delta: ${transcript}`);
        session.userTranscriptBuffer += transcript;
        session.totalUserTranscriptLength += transcript.length; // ëˆ„ì  ê¸¸ì´ ì¶”ì 
      }

      // Handle output transcription (AI speech) - í† í° ì¶”ì ì€ ì—¬ê¸°ì„œë§Œ ìˆ˜í–‰
      // modelTurn.parts.textì™€ outputTranscription.textê°€ ë™ì¼ ë‚´ìš©ì´ë¯€ë¡œ ì—¬ê¸°ì„œë§Œ ì¶”ì 
      if (serverContent.outputTranscription) {
        const transcript = serverContent.outputTranscription.text || '';
        console.log(`ğŸ¤– AI transcript delta: ${transcript}`);
        // currentTranscriptëŠ” modelTurnì—ì„œ ì´ë¯¸ ëˆ„ì ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê¸¸ì´ë§Œ ì¶”ì 
        if (!serverContent.modelTurn) {
          session.currentTranscript += transcript;
        }
        session.totalAiTranscriptLength += transcript.length; // ëˆ„ì  ê¸¸ì´ ì¶”ì  (ì—¬ê¸°ì„œë§Œ)
      }
    }
  }

  handleClientMessage(sessionId: string, message: any): void {
    const session = this.sessions.get(sessionId);
    if (!session) {
      console.error(`Session not found: ${sessionId}`);
      return;
    }

    if (!session.isConnected || !session.geminiSession) {
      console.error(`Gemini not connected for session: ${sessionId}`);
      return;
    }

    // Forward client messages to Gemini
    switch (message.type) {
      case 'input_audio_buffer.append':
        // Client sending audio data (base64 PCM16)
        // Gemini expects 16kHz PCM16
        const audioLength = message.audio ? message.audio.length : 0;
        console.log(`ğŸ¤ Received audio chunk: ${audioLength} bytes (base64)`);
        session.geminiSession.sendRealtimeInput({
          audio: {
            data: message.audio,
            mimeType: 'audio/pcm;rate=16000',
          },
        });
        break;

      case 'input_audio_buffer.commit':
        // User stopped recording - send END_OF_TURN event to Gemini
        // Note: transcript will be sent automatically when Gemini detects turn completion via VAD
        console.log('ğŸ“¤ User stopped recording, sending END_OF_TURN event');
        session.geminiSession.sendRealtimeInput({
          event: 'END_OF_TURN'
        });
        break;

      case 'response.create':
        // Client explicitly requesting a response - send END_OF_TURN to trigger Gemini
        console.log('ğŸ”„ Explicit response request, sending END_OF_TURN event');
        session.geminiSession.sendRealtimeInput({
          event: 'END_OF_TURN'
        });
        break;

      case 'conversation.item.create':
        // Client sending a text message
        if (message.item && message.item.content) {
          const text = message.item.content[0]?.text || '';
          session.geminiSession.sendClientContent({
            turns: [{ role: 'user', parts: [{ text }] }],
            turnComplete: true,
          });
        }
        break;

      default:
        console.log(`Unknown client message type: ${message.type}`);
    }
  }

  private async analyzeEmotion(aiResponse: string, personaName: string): Promise<{ emotion: string; emotionReason: string }> {
    if (!this.genAI) {
      return { emotion: 'ì¤‘ë¦½', emotionReason: 'ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.' };
    }

    try {
      const result = await this.genAI.models.generateContent({
        model: 'gemini-2.0-flash-exp',
        contents: `ë‹¤ìŒ AI ìºë¦­í„°(${personaName})ì˜ ì‘ë‹µì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”.\n\nì‘ë‹µ: "${aiResponse}"\n\nê°ì •ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤: ì¤‘ë¦½, ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë†€ëŒ, í˜¸ê¸°ì‹¬, ë¶ˆì•ˆ, í”¼ë¡œ, ì‹¤ë§, ë‹¹í˜¹\nê°ì • ì´ìœ ëŠ” ê°„ë‹¨í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.`,
        config: {
          responseMimeType: "application/json",
          responseSchema: {
            type: "object",
            properties: {
              emotion: { type: "string" },
              emotionReason: { type: "string" }
            },
            required: ["emotion", "emotionReason"]
          },
          maxOutputTokens: 200,
          temperature: 0.5
        }
      });

      const responseText = result.text || '{}';
      console.log('ğŸ“Š Gemini emotion analysis response:', responseText);
      const emotionData = JSON.parse(responseText);

      return {
        emotion: emotionData.emotion || 'ì¤‘ë¦½',
        emotionReason: emotionData.emotionReason || 'ê°ì • ë¶„ì„ ì‹¤íŒ¨'
      };
    } catch (error) {
      console.error('âŒ Emotion analysis error:', error);
      return { emotion: 'ì¤‘ë¦½', emotionReason: 'ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' };
    }
  }

  private sendToClient(session: RealtimeSession, message: any): void {
    if (session.clientWs && session.clientWs.readyState === WebSocket.OPEN) {
      session.clientWs.send(JSON.stringify(message));
    }
  }

  // ì„¸ì…˜ ì‚¬ìš©ëŸ‰ ì¶”ì  í—¬í¼ ë©”ì„œë“œ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ í•œ ë²ˆë§Œ í˜¸ì¶œ)
  private trackSessionUsage(session: RealtimeSession): void {
    // ì´ë¯¸ ì¶”ì ëœ ì„¸ì…˜ì¸ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
    if ((session as any)._usageTracked) {
      return;
    }
    (session as any)._usageTracked = true;
    
    const durationMs = Date.now() - session.startTime;
    
    // í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í† í° ì¶”ì • (í•œêµ­ì–´: ì•½ 2-3ì = 1í† í°)
    const estimatedUserTokens = Math.ceil(session.totalUserTranscriptLength / 2);
    const estimatedAiTokens = Math.ceil(session.totalAiTranscriptLength / 2);
    
    // Gemini Live APIëŠ” ìŒì„± ì²˜ë¦¬ë„ í•¨ê»˜ í•˜ë¯€ë¡œ í…ìŠ¤íŠ¸ í† í°ì˜ ì•½ 1.5ë°° ì¶”ì •
    // (í…ìŠ¤íŠ¸ë§Œ ê³ ë ¤í•˜ë©´ ê³¼ì†Œí‰ê°€, ì˜¤ë””ì˜¤ ì „ë¶€ ê³„ì‚°í•˜ë©´ ê³¼ëŒ€í‰ê°€)
    const audioTokenMultiplier = 1.5;
    const totalPromptTokens = Math.ceil(estimatedUserTokens * audioTokenMultiplier);
    const totalCompletionTokens = Math.ceil(estimatedAiTokens * audioTokenMultiplier);
    
    if (totalPromptTokens > 0 || totalCompletionTokens > 0) {
      trackUsage({
        feature: 'realtime',
        model: session.realtimeModel,
        provider: 'gemini',
        promptTokens: totalPromptTokens,
        completionTokens: totalCompletionTokens,
        userId: session.userId,
        conversationId: session.conversationId,
        durationMs,
        metadata: {
          scenarioId: session.scenarioId,
          personaId: session.personaId,
          totalUserTranscriptLength: session.totalUserTranscriptLength,
          totalAiTranscriptLength: session.totalAiTranscriptLength,
          estimationMethod: 'transcript_length_based',
        }
      });
      
      console.log(`ğŸ“Š Realtime usage tracked: ${totalPromptTokens} prompt + ${totalCompletionTokens} completion tokens, duration: ${Math.round(durationMs/1000)}s`);
    }
  }

  closeSession(sessionId: string): void {
    const session = this.sessions.get(sessionId);
    if (session) {
      console.log(`ğŸ”š Closing realtime voice session: ${sessionId}`);
      
      // ì„¸ì…˜ ì‚¬ìš©ëŸ‰ ì¶”ì 
      this.trackSessionUsage(session);
      
      if (session.geminiSession) {
        session.geminiSession.close();
      }
      
      this.sessions.delete(sessionId);
    }
  }

  getActiveSessionCount(): number {
    return this.sessions.size;
  }
}

export const realtimeVoiceService = new RealtimeVoiceService();
