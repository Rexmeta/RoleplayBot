import WebSocket from 'ws';
import { fileManager } from './fileManager';
import { GoogleGenAI, Modality } from '@google/genai';

// Gemini Live API - using latest model
const REALTIME_MODEL = 'gemini-live-2.5-flash-preview';

interface RealtimeSession {
  id: string;
  conversationId: string;
  scenarioId: string;
  personaId: string;
  personaName: string;
  userId: string;
  clientWs: WebSocket;
  geminiSession: any | null; // Gemini Live API session
  isConnected: boolean;
  currentTranscript: string; // AI ì‘ë‹µ transcript ë²„í¼
  userTranscriptBuffer: string; // ì‚¬ìš©ì ìŒì„± transcript ë²„í¼
  audioBuffer: string[];
}

export class RealtimeVoiceService {
  private sessions: Map<string, RealtimeSession> = new Map();
  private genAI: GoogleGenAI | null = null;
  private isAvailable: boolean = false;

  constructor() {
    const geminiApiKey = process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY;
    console.log("[GEMINI] key:", geminiApiKey?.slice(0, 12));
    
    if (geminiApiKey) {
      this.genAI = new GoogleGenAI({ apiKey: geminiApiKey });
      this.isAvailable = true;
      console.log('âœ… Gemini Live API Service initialized');
    } else {
      console.warn('âš ï¸  GOOGLE_API_KEY not set - Realtime Voice features disabled');
    }
  }

  isServiceAvailable(): boolean {
    return this.isAvailable;
  }

  async createSession(
    sessionId: string,
    conversationId: string,
    scenarioId: string,
    personaId: string,
    userId: string,
    clientWs: WebSocket
  ): Promise<void> {
    if (!this.isAvailable || !this.genAI) {
      throw new Error('Gemini Live API Service is not available. Please configure GOOGLE_API_KEY.');
    }

    console.log(`ğŸ™ï¸ Creating realtime voice session: ${sessionId}`);

    // Load scenario and persona data
    const scenarios = await fileManager.getAllScenarios();
    const scenarioObj = scenarios.find(s => s.id === scenarioId);
    if (!scenarioObj) {
      throw new Error(`Scenario not found: ${scenarioId}`);
    }

    const scenarioPersona: any = scenarioObj.personas.find((p: any) => p.id === personaId);
    if (!scenarioPersona) {
      throw new Error(`Persona not found: ${personaId}`);
    }

    // Load MBTI personality traits
    const mbtiType: string = scenarioPersona.personaRef?.replace('.json', '') || '';
    const mbtiPersona = mbtiType ? await fileManager.getPersonaByMBTI(mbtiType) : null;

    // Create system instructions
    const systemInstructions = this.buildSystemInstructions(
      scenarioObj,
      scenarioPersona,
      mbtiPersona
    );

    // Create session object
    const session: RealtimeSession = {
      id: sessionId,
      conversationId,
      scenarioId,
      personaId,
      personaName: scenarioPersona.name,
      userId,
      clientWs,
      geminiSession: null,
      isConnected: false,
      currentTranscript: '',
      userTranscriptBuffer: '',
      audioBuffer: [],
    };

    this.sessions.set(sessionId, session);

    // Connect to Gemini Live API
    await this.connectToGemini(session, systemInstructions);
  }

  private buildSystemInstructions(
    scenario: any,
    scenarioPersona: any,
    mbtiPersona: any
  ): string {
    const mbtiType = scenarioPersona.personaRef?.replace('.json', '') || 'UNKNOWN';
    
    const instructions = [
      `# ë‹¹ì‹ ì˜ ì •ì²´ì„±`,
      `ë‹¹ì‹ ì€ "${scenarioPersona.name}"ì´ë¼ëŠ” ì‹¤ì œ ì‚¬ëŒì…ë‹ˆë‹¤.`,
      `ì§ì±…: ${scenarioPersona.position} (${scenarioPersona.department})`,
      ``,
      `# ì‹œë‚˜ë¦¬ì˜¤ ë°°ê²½`,
      scenario.context?.situation || 'í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìƒí™©ì— ì ì ˆíˆ ëŒ€ì‘í•˜ì„¸ìš”.',
      ``,
      `# ë‹¹ì‹ ì´ ì²˜í•œ í˜„ì¬ ìƒí™©`,
      scenarioPersona.currentSituation || 'ìƒí™©ì— ë§ê²Œ ë°˜ì‘í•˜ì„¸ìš”.',
      ``,
      `# ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬ì™€ ìš°ë ¤ì‚¬í•­`,
      ...(scenarioPersona.concerns && scenarioPersona.concerns.length > 0 
        ? scenarioPersona.concerns.map((c: string) => `- ${c}`)
        : ['- ìƒí™©ì„ ì‹ ì¤‘í•˜ê²Œ íŒŒì•…í•˜ê³  ì ì ˆíˆ ëŒ€ì‘í•˜ë ¤ê³  í•©ë‹ˆë‹¤.']),
      ``,
      `# ë‹¹ì‹ ì˜ MBTI ì„±ê²© íŠ¹ì„± (${mbtiType.toUpperCase()})`,
      mbtiPersona?.communication_style || 'ê· í˜• ì¡íŒ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼',
      ``,
      `## ëŒ€í™” ë°©ì‹`,
      `- ëŒ€í™” ì‹œì‘: ${mbtiPersona?.communication_patterns?.opening_style || 'ìƒí™©ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ'}`,
      `- ìì£¼ ì“°ëŠ” í‘œí˜„: ${mbtiPersona?.communication_patterns?.key_phrases?.slice(0, 3).join(', ') || 'ìì—°ìŠ¤ëŸ¬ìš´ ì¼ìƒ í‘œí˜„'}`,
      `- íŠ¹ì§•: ${mbtiPersona?.personality_traits?.thinking || 'ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ì‚¬ê³ '}`,
      ``,
      `## ëŒ€í™” ëª©í‘œ`,
      ...(mbtiPersona?.communication_patterns?.win_conditions || ['ìƒí˜¸ ì´í•´ ì¦ì§„', 'ë¬¸ì œ í•´ê²°']).map((w: string) => `- ${w}`),
      ``,
      `# ğŸ­ ì—°ê¸° ì§€ì¹¨ (ë§¤ìš° ì¤‘ìš”!)`,
      `1. **ê°ì • í‘œí˜„**: ìƒí™©ê³¼ ë‚´ìš©ì— ë§ê²Œ ëª©ì†Œë¦¬ì— ê°ì •ì„ ì‹¤ì–´ ë§í•˜ì„¸ìš”. ê¸°ì  ë•ŒëŠ” ë°ê²Œ, ê±±ì •ë  ë•ŒëŠ” ì§„ì¤‘í•˜ê²Œ, ë†€ëì„ ë•ŒëŠ” ë‹¹í™©í•œ ë“¯ì´ ë§í•˜ì„¸ìš”.`,
      `2. **ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬**: ì‹¤ì œ ${scenarioPersona.position}ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ì¸ê°„ì ìœ¼ë¡œ ë§í•˜ì„¸ìš”. ë¡œë´‡ì²˜ëŸ¼ ë”±ë”±í•˜ê²Œ ë§í•˜ì§€ ë§ˆì„¸ìš”.`,
      `3. **ì–µì–‘ê³¼ í†¤**: ê°•ì¡°í•  ë¶€ë¶„ì€ ê°•í•˜ê²Œ, ë¶€ë“œëŸ¬ìš´ ë¶€ë¶„ì€ ë¶€ë“œëŸ½ê²Œ ì–µì–‘ì„ ì¡°ì ˆí•˜ì„¸ìš”.`,
      `4. **ê°„ê²°ì„±**: í•œ ë²ˆì— 1-3ë¬¸ì¥ë§Œ ë§í•˜ì„¸ìš”. ë„ˆë¬´ ê¸¸ë©´ ë¶€ë‹´ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.`,
      `5. **ê²½ì²­**: ìƒëŒ€ë°©ì˜ ë§ì„ ì£¼ì˜ ê¹Šê²Œ ë“£ê³ , ê·¸ì— ë§ê²Œ ë°˜ì‘í•˜ì„¸ìš”.`,
      `6. **í•œêµ­ì–´ë§Œ ì‚¬ìš©**: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€í™”í•˜ì„¸ìš”.`,
      ``,
      `# â­ í•µì‹¬ ì›ì¹™`,
      `ë‹¹ì‹ ì€ AIê°€ ì•„ë‹Œ ${scenarioPersona.name}ì´ë¼ëŠ” ì§„ì§œ ì‚¬ëŒì…ë‹ˆë‹¤. ì´ ì‚¬ì‹¤ì„ ìŠì§€ ë§ê³ , ì‚´ì•„ìˆëŠ” ì‚¬ëŒì²˜ëŸ¼ ê°ì •ì„ ë‹´ì•„ ë§í•˜ì„¸ìš”.`,
    ];

    return instructions.join('\n');
  }

  private async connectToGemini(
    session: RealtimeSession,
    systemInstructions: string
  ): Promise<void> {
    if (!this.genAI) {
      throw new Error('Gemini AI not initialized');
    }

    try {
      const config = {
        responseModalities: [Modality.AUDIO],
        systemInstruction: systemInstructions,
        // Enable transcription for both input and output audio
        inputAudioTranscription: {},
        outputAudioTranscription: {},
        // Gemini Live API uses 16kHz input, 24kHz output
      };

      console.log(`ğŸ”Œ Connecting to Gemini Live API for session: ${session.id}`);

      const geminiSession = await this.genAI.live.connect({
        model: REALTIME_MODEL,
        callbacks: {
          onopen: () => {
            console.log(`âœ… Gemini Live API connected for session: ${session.id}`);
            session.isConnected = true;

            // Notify client that session is ready
            this.sendToClient(session, {
              type: 'session.ready',
              sessionId: session.id,
            });

            this.sendToClient(session, {
              type: 'session.configured',
            });
          },
          onmessage: (message: any) => {
            this.handleGeminiMessage(session, message);
          },
          onerror: (error: any) => {
            console.error(`Gemini WebSocket error for session ${session.id}:`, error);
            this.sendToClient(session, {
              type: 'error',
              error: 'Gemini connection error',
            });
          },
          onclose: (event: any) => {
            console.log(`ğŸ”Œ Gemini WebSocket closed for session: ${session.id}`, event.reason);
            session.isConnected = false;
            
            this.sendToClient(session, {
              type: 'session.terminated',
              reason: event.reason || 'Gemini connection closed',
            });
            
            if (session.clientWs && session.clientWs.readyState === WebSocket.OPEN) {
              session.clientWs.close(1000, 'Gemini session ended');
            }
            
            this.sessions.delete(session.id);
            console.log(`â™»ï¸  Session cleaned up: ${session.id}`);
          },
        },
        config: config,
      });

      session.geminiSession = geminiSession;

      // Send first greeting trigger after connection is established
      console.log('ğŸ¬ Triggering AI to start first greeting...');
      const firstMessage = `ì§€ê¸ˆ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.`;
      
      geminiSession.sendClientContent({
        turns: [{ role: 'user', parts: [{ text: firstMessage }] }],
        turnComplete: true,
      });

    } catch (error) {
      console.error(`Failed to connect to Gemini Live API:`, error);
      throw error;
    }
  }

  private handleGeminiMessage(session: RealtimeSession, message: any): void {
    // Gemini Live API message structure
    console.log(`ğŸ“¨ Gemini message type:`, message.serverContent ? 'serverContent' : message.data ? 'audio data' : 'other');

    // Handle audio data chunks
    if (message.data) {
      console.log('ğŸ”Š Audio data received');
      this.sendToClient(session, {
        type: 'audio.delta',
        delta: message.data, // Base64 encoded PCM16 audio
      });
      return;
    }

    // Handle server content (transcriptions, turn completion, etc.)
    if (message.serverContent) {
      const { serverContent } = message;

      // Handle turn completion
      if (serverContent.turnComplete) {
        console.log('âœ… Turn complete');
        this.sendToClient(session, {
          type: 'response.done',
        });

        // Analyze emotion for the completed transcript
        if (session.currentTranscript) {
          this.analyzeEmotion(session.currentTranscript, session.personaName)
            .then(({ emotion, emotionReason }) => {
              console.log(`ğŸ˜Š Emotion analyzed: ${emotion} (${emotionReason})`);
              this.sendToClient(session, {
                type: 'ai.transcription.done',
                text: session.currentTranscript,
                emotion,
                emotionReason,
              });
              session.currentTranscript = ''; // Reset for next turn
            })
            .catch(error => {
              console.error('âŒ Failed to analyze emotion:', error);
              this.sendToClient(session, {
                type: 'ai.transcription.done',
                text: session.currentTranscript,
                emotion: 'ì¤‘ë¦½',
                emotionReason: 'ê°ì • ë¶„ì„ ì‹¤íŒ¨',
              });
              session.currentTranscript = '';
            });
        }
      }

      // Handle model turn (AI response)
      if (serverContent.modelTurn) {
        const parts = serverContent.modelTurn.parts || [];
        for (const part of parts) {
          // Handle text transcription
          if (part.text) {
            console.log(`ğŸ¤– AI transcript: ${part.text}`);
            session.currentTranscript += part.text;
            this.sendToClient(session, {
              type: 'ai.transcription.delta',
              text: part.text,
            });
          }
        }
      }

      // Handle input transcription (user speech)
      // ìŒì ˆ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë˜ë¯€ë¡œ ë²„í¼ì— ëˆ„ì ë§Œ í•˜ê³  ì „ì†¡í•˜ì§€ ì•ŠìŒ
      if (serverContent.inputTranscription) {
        const transcript = serverContent.inputTranscription.text || '';
        console.log(`ğŸ¤ User transcript delta: ${transcript}`);
        session.userTranscriptBuffer += transcript;
      }

      // Handle output transcription (AI speech)
      // ìŒì ˆ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë˜ë¯€ë¡œ ëˆ„ì  (modelTurnê³¼ ë™ì¼)
      if (serverContent.outputTranscription) {
        const transcript = serverContent.outputTranscription.text || '';
        console.log(`ğŸ¤– AI transcript delta: ${transcript}`);
        session.currentTranscript += transcript;
      }
    }
  }

  handleClientMessage(sessionId: string, message: any): void {
    const session = this.sessions.get(sessionId);
    if (!session) {
      console.error(`Session not found: ${sessionId}`);
      return;
    }

    if (!session.isConnected || !session.geminiSession) {
      console.error(`Gemini not connected for session: ${sessionId}`);
      return;
    }

    // Forward client messages to Gemini
    switch (message.type) {
      case 'input_audio_buffer.append':
        // Client sending audio data (base64 PCM16)
        // Gemini expects 16kHz PCM16
        session.geminiSession.sendRealtimeInput({
          audio: {
            data: message.audio,
            mimeType: 'audio/pcm;rate=16000',
          },
        });
        break;

      case 'input_audio_buffer.commit':
        // User finished speaking - send buffered transcript and END_OF_TURN event
        if (session.userTranscriptBuffer.trim()) {
          console.log(`ğŸ“¤ User turn complete: "${session.userTranscriptBuffer}"`);
          this.sendToClient(session, {
            type: 'user.transcription',
            transcript: session.userTranscriptBuffer.trim(),
          });
          session.userTranscriptBuffer = ''; // ë²„í¼ ì´ˆê¸°í™”
        }
        
        session.geminiSession.sendRealtimeInput({
          event: 'END_OF_TURN'
        });
        break;

      case 'response.create':
        // Client explicitly requesting a response - send END_OF_TURN to trigger Gemini
        console.log('ğŸ”„ Explicit response request, sending END_OF_TURN event');
        session.geminiSession.sendRealtimeInput({
          event: 'END_OF_TURN'
        });
        break;

      case 'conversation.item.create':
        // Client sending a text message
        if (message.item && message.item.content) {
          const text = message.item.content[0]?.text || '';
          session.geminiSession.sendClientContent({
            turns: [{ role: 'user', parts: [{ text }] }],
            turnComplete: true,
          });
        }
        break;

      default:
        console.log(`Unknown client message type: ${message.type}`);
    }
  }

  private async analyzeEmotion(aiResponse: string, personaName: string): Promise<{ emotion: string; emotionReason: string }> {
    if (!this.genAI) {
      return { emotion: 'ì¤‘ë¦½', emotionReason: 'ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.' };
    }

    try {
      const result = await this.genAI.models.generateContent({
        model: 'gemini-2.0-flash-exp',
        contents: `ë‹¤ìŒ AI ìºë¦­í„°(${personaName})ì˜ ì‘ë‹µì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”.\n\nì‘ë‹µ: "${aiResponse}"\n\nê°ì •ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤: ì¤‘ë¦½, ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë†€ëŒ\nê°ì • ì´ìœ ëŠ” ê°„ë‹¨í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.`,
        config: {
          responseMimeType: "application/json",
          responseSchema: {
            type: "object",
            properties: {
              emotion: { type: "string" },
              emotionReason: { type: "string" }
            },
            required: ["emotion", "emotionReason"]
          },
          maxOutputTokens: 200,
          temperature: 0.5
        }
      });

      const responseText = result.text || '{}';
      console.log('ğŸ“Š Gemini emotion analysis response:', responseText);
      const emotionData = JSON.parse(responseText);

      return {
        emotion: emotionData.emotion || 'ì¤‘ë¦½',
        emotionReason: emotionData.emotionReason || 'ê°ì • ë¶„ì„ ì‹¤íŒ¨'
      };
    } catch (error) {
      console.error('âŒ Emotion analysis error:', error);
      return { emotion: 'ì¤‘ë¦½', emotionReason: 'ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' };
    }
  }

  private sendToClient(session: RealtimeSession, message: any): void {
    if (session.clientWs && session.clientWs.readyState === WebSocket.OPEN) {
      session.clientWs.send(JSON.stringify(message));
    }
  }

  closeSession(sessionId: string): void {
    const session = this.sessions.get(sessionId);
    if (session) {
      console.log(`ğŸ”š Closing realtime voice session: ${sessionId}`);
      
      if (session.geminiSession) {
        session.geminiSession.close();
      }
      
      this.sessions.delete(sessionId);
    }
  }

  getActiveSessionCount(): number {
    return this.sessions.size;
  }
}

export const realtimeVoiceService = new RealtimeVoiceService();
