import WebSocket from 'ws';
import { fileManager } from './fileManager';
import { GoogleGenAI, Modality } from '@google/genai';
import { getRealtimeVoiceGuidelines, validateDifficultyLevel } from './conversationDifficultyPolicy';
import { storage } from '../storage';
import { trackUsage } from './aiUsageTracker';

// Default Gemini Live API model (updated December 2025)
const DEFAULT_REALTIME_MODEL = 'gemini-2.5-flash-native-audio-preview-09-2025';

// Geminiì˜ thinking/reasoning í…ìŠ¤íŠ¸ë¥¼ í•„í„°ë§í•˜ê³  í•œêµ­ì–´ ì‘ë‹µë§Œ ì¶”ì¶œ
function filterThinkingText(text: string): string {
  if (!text) return '';
  
  // íŒ¨í„´ 1: **ì œëª©** í˜•ì‹ì˜ thinking ë¸”ë¡ ì œê±°
  // ì˜ˆ: "**Beginning the Briefing**\nI've initiated..."
  let filtered = text.replace(/\*\*[^*]+\*\*\s*/g, '');
  
  // íŒ¨í„´ 2: ì˜ë¬¸ìœ¼ë¡œë§Œ êµ¬ì„±ëœ ì¤„ ì œê±° (í•œê¸€ì´ ì—†ëŠ” ì¤„)
  const lines = filtered.split('\n');
  const koreanLines = lines.filter(line => {
    const trimmed = line.trim();
    if (!trimmed) return false;
    // í•œê¸€ì´ í¬í•¨ëœ ì¤„ë§Œ ìœ ì§€
    return /[\uAC00-\uD7AF\u1100-\u11FF\u3130-\u318F]/.test(trimmed);
  });
  
  filtered = koreanLines.join('\n').trim();
  
  // íŒ¨í„´ 3: ë‚¨ì€ í…ìŠ¤íŠ¸ì—ì„œ ì•ë’¤ ê³µë°± ì •ë¦¬
  return filtered;
}

// ë™ì‹œ ì ‘ì† ìµœì í™” ì„¤ì •
const SESSION_TIMEOUT_MS = 30 * 60 * 1000; // 30ë¶„ ë¹„í™œì„± íƒ€ì„ì•„ì›ƒ
const MAX_TRANSCRIPT_LENGTH = 50000; // íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ìµœëŒ€ ê¸¸ì´ (ì•½ 25,000ì)
const CLEANUP_INTERVAL_MS = 60 * 1000; // 1ë¶„ë§ˆë‹¤ ì •ë¦¬

interface RealtimeSession {
  id: string;
  conversationId: string;
  scenarioId: string;
  personaId: string;
  personaName: string;
  userId: string;
  clientWs: WebSocket;
  geminiSession: any | null; // Gemini Live API session
  isConnected: boolean;
  currentTranscript: string; // AI ì‘ë‹µ transcript ë²„í¼
  userTranscriptBuffer: string; // ì‚¬ìš©ì ìŒì„± transcript ë²„í¼
  audioBuffer: string[];
  startTime: number; // ì„¸ì…˜ ì‹œì‘ ì‹œê°„ (ms)
  lastActivityTime: number; // ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ (ms)
  totalUserTranscriptLength: number; // ëˆ„ì  ì‚¬ìš©ì í…ìŠ¤íŠ¸ ê¸¸ì´
  totalAiTranscriptLength: number; // ëˆ„ì  AI í…ìŠ¤íŠ¸ ê¸¸ì´
  realtimeModel: string; // ì‚¬ìš©ëœ ëª¨ë¸
  hasReceivedFirstAIResponse: boolean; // ì²« AI ì‘ë‹µ ìˆ˜ì‹  ì—¬ë¶€
  firstGreetingRetryCount: number; // ì²« ì¸ì‚¬ ì¬ì‹œë„ íšŸìˆ˜
  isInterrupted: boolean; // Barge-in flag to suppress audio until new response
  turnSeq: number; // Monotonic turn counter, incremented on each turnComplete
  cancelledTurnSeq: number; // Turn seq when cancel was issued (ignore audio from this turn)
}

export class RealtimeVoiceService {
  private sessions: Map<string, RealtimeSession> = new Map();
  private genAI: GoogleGenAI | null = null;
  private isAvailable: boolean = false;
  private cleanupInterval: NodeJS.Timeout | null = null;

  constructor() {
    const geminiApiKey = process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY;
    
    if (geminiApiKey) {
      this.genAI = new GoogleGenAI({ apiKey: geminiApiKey });
      this.isAvailable = true;
      console.log('âœ… Gemini Live API Service initialized');
      
      // ë¹„í™œì„± ì„¸ì…˜ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
      this.startCleanupScheduler();
    } else {
      console.warn('âš ï¸  GOOGLE_API_KEY not set - Realtime Voice features disabled');
    }
  }
  
  // ë¹„í™œì„± ì„¸ì…˜ ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬
  private startCleanupScheduler(): void {
    this.cleanupInterval = setInterval(() => {
      this.cleanupInactiveSessions();
    }, CLEANUP_INTERVAL_MS);
    
    console.log(`ğŸ§¹ Session cleanup scheduler started (interval: ${CLEANUP_INTERVAL_MS / 1000}s)`);
  }
  
  // ë¹„í™œì„± ì„¸ì…˜ ì •ë¦¬
  private cleanupInactiveSessions(): void {
    const now = Date.now();
    const sessionsToClose: string[] = [];
    
    this.sessions.forEach((session, sessionId) => {
      const inactiveTime = now - session.lastActivityTime;
      
      // íƒ€ì„ì•„ì›ƒëœ ì„¸ì…˜ ì‹ë³„
      if (inactiveTime > SESSION_TIMEOUT_MS) {
        console.log(`â° Session ${sessionId} inactive for ${Math.round(inactiveTime / 60000)}min, marking for cleanup`);
        sessionsToClose.push(sessionId);
      }
    });
    
    // ì„¸ì…˜ ì •ë¦¬
    for (const sessionId of sessionsToClose) {
      this.closeSession(sessionId);
    }
    
    if (sessionsToClose.length > 0) {
      console.log(`ğŸ§¹ Cleaned up ${sessionsToClose.length} inactive sessions. Active: ${this.sessions.size}`);
    }
  }

  isServiceAvailable(): boolean {
    return this.isAvailable;
  }

  private async getRealtimeModel(): Promise<string> {
    try {
      // Add timeout to prevent blocking WebSocket connection
      const timeoutPromise = new Promise<undefined>((_, reject) => 
        setTimeout(() => reject(new Error('DB setting fetch timeout')), 2000)
      );
      
      const settingPromise = storage.getSystemSetting("ai", "model_realtime");
      const setting = await Promise.race([settingPromise, timeoutPromise]);
      
      // Validate the model value is a valid Gemini Live model
      const validModels = [
        'gemini-2.5-flash-native-audio-preview-09-2025'
      ];
      
      const model = setting?.value;
      if (model && validModels.includes(model)) {
        console.log(`ğŸ¤– Using realtime model from DB: ${model}`);
        return model;
      }
      
      console.log(`ğŸ¤– Using default realtime model: ${DEFAULT_REALTIME_MODEL}`);
      return DEFAULT_REALTIME_MODEL;
    } catch (error) {
      console.warn(`âš ï¸ Failed to get realtime model from DB, using default: ${DEFAULT_REALTIME_MODEL}`);
      return DEFAULT_REALTIME_MODEL;
    }
  }

  async createSession(
    sessionId: string,
    conversationId: string,
    scenarioId: string,
    personaId: string,
    userId: string,
    clientWs: WebSocket,
    userSelectedDifficulty?: number // ì‚¬ìš©ìê°€ ì„ íƒí•œ ë‚œì´ë„ (1-4)
  ): Promise<void> {
    if (!this.isAvailable || !this.genAI) {
      throw new Error('Gemini Live API Service is not available. Please configure GOOGLE_API_KEY.');
    }

    console.log(`ğŸ™ï¸ Creating realtime voice session: ${sessionId}`);

    // Load scenario and persona data
    const scenarios = await fileManager.getAllScenarios();
    const scenarioObj = scenarios.find(s => s.id === scenarioId);
    if (!scenarioObj) {
      throw new Error(`Scenario not found: ${scenarioId}`);
    }

    const scenarioPersona: any = scenarioObj.personas.find((p: any) => p.id === personaId);
    if (!scenarioPersona) {
      throw new Error(`Persona not found: ${personaId}`);
    }

    // Load MBTI personality traits
    const mbtiType: string = scenarioPersona.personaRef?.replace('.json', '') || '';
    const mbtiPersona = mbtiType ? await fileManager.getPersonaByMBTI(mbtiType) : null;

    // ì‚¬ìš©ì ì •ë³´ ë¡œë“œ (ì´ë¦„, ì—­í• )
    let userName = 'ì‚¬ìš©ì';
    try {
      const user = await storage.getUser(userId);
      if (user?.name) {
        userName = user.name;
      }
    } catch (error) {
      console.warn(`âš ï¸ Failed to load user info for userId ${userId}:`, error);
    }

    // ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì‚¬ìš©ì ì—­í•  ì •ë³´ ì¶”ì¶œ
    const playerRole = scenarioObj.context?.playerRole || {};
    const userRoleInfo = {
      name: userName,
      position: playerRole.position || 'ë‹´ë‹¹ì',
      department: playerRole.department || '',
      experience: playerRole.experience || '',
      responsibility: playerRole.responsibility || ''
    };
    
    console.log(`ğŸ‘¤ ì‚¬ìš©ì ì •ë³´: ${userRoleInfo.name} (${userRoleInfo.position}${userRoleInfo.department ? ', ' + userRoleInfo.department : ''})`);

    // ì‚¬ìš©ìê°€ ì„ íƒí•œ ë‚œì´ë„ë¥¼ ì‹œë‚˜ë¦¬ì˜¤ ê°ì²´ì— ì ìš©
    const scenarioWithUserDifficulty = {
      ...scenarioObj,
      difficulty: userSelectedDifficulty || 2 // ì‚¬ìš©ìê°€ ì„ íƒí•œ ë‚œì´ë„ ì‚¬ìš©, ê¸°ë³¸ê°’ 2
    };

    // Create system instructions
    const systemInstructions = this.buildSystemInstructions(
      scenarioWithUserDifficulty,
      scenarioPersona,
      mbtiPersona,
      userRoleInfo
    );

    console.log('\n' + '='.repeat(80));
    console.log('ğŸ¯ ì‹¤ì‹œê°„ ëŒ€í™” ì‹œì‘ - ì „ë‹¬ë˜ëŠ” ëª…ë ¹ ë° ì»¨í…ìŠ¤íŠ¸');
    console.log('='.repeat(80));
    console.log('ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤:', scenarioObj.title);
    console.log('ğŸ‘¤ í˜ë¥´ì†Œë‚˜:', scenarioPersona.name, `(${scenarioPersona.position})`);
    console.log('ğŸ­ MBTI:', mbtiType.toUpperCase());
    console.log('='.repeat(80));
    console.log('ğŸ“ ì‹œìŠ¤í…œ ëª…ë ¹ (SYSTEM INSTRUCTIONS):\n');
    console.log(systemInstructions);
    console.log('='.repeat(80) + '\n');

    // Get realtime model for tracking
    const realtimeModel = await this.getRealtimeModel();

    // Create session object
    const session: RealtimeSession = {
      id: sessionId,
      conversationId,
      scenarioId,
      personaId,
      personaName: scenarioPersona.name,
      userId,
      clientWs,
      geminiSession: null,
      isConnected: false,
      currentTranscript: '',
      userTranscriptBuffer: '',
      audioBuffer: [],
      startTime: Date.now(),
      lastActivityTime: Date.now(),
      totalUserTranscriptLength: 0,
      totalAiTranscriptLength: 0,
      realtimeModel,
      hasReceivedFirstAIResponse: false,
      firstGreetingRetryCount: 0,
      isInterrupted: false,
      turnSeq: 0, // First turn is 0
      cancelledTurnSeq: -1, // No cancelled turn initially
    };

    this.sessions.set(sessionId, session);

    // ì„±ë³„ íŒë‹¨ (ì‹œë‚˜ë¦¬ì˜¤ í˜ë¥´ì†Œë‚˜ì˜ gender ì†ì„± ì‚¬ìš©)
    const gender: 'male' | 'female' = scenarioPersona.gender === 'female' ? 'female' : 'male';
    console.log(`ğŸ‘¤ í˜ë¥´ì†Œë‚˜ ì„±ë³„ ì„¤ì •: ${scenarioPersona.name} â†’ ${gender} (ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜ê°’: ${scenarioPersona.gender})`);
    
    // Connect to Gemini Live API
    await this.connectToGemini(session, systemInstructions, gender);
  }

  private buildSystemInstructions(
    scenario: any,
    scenarioPersona: any,
    mbtiPersona: any,
    userRoleInfo?: { name: string; position: string; department: string; experience: string; responsibility: string }
  ): string {
    const mbtiType = scenarioPersona.personaRef?.replace('.json', '') || 'UNKNOWN';
    
    // ëŒ€í™” ë‚œì´ë„ ë ˆë²¨ ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ìê°€ ì„ íƒí•œ ë‚œì´ë„ ì‚¬ìš©, ê¸°ë³¸ê°’ 2)
    const difficultyLevel = validateDifficultyLevel(scenario.difficulty);
    console.log(`ğŸ¯ ëŒ€í™” ë‚œì´ë„: Level ${difficultyLevel} (ì‚¬ìš©ì ì„ íƒ)`)
    
    const difficultyGuidelines = getRealtimeVoiceGuidelines(difficultyLevel);
    
    // ëŒ€í™” ìƒëŒ€(ì‚¬ìš©ì) ì •ë³´ ì„¹ì…˜ êµ¬ì„±
    const userInfoSection = userRoleInfo ? [
      `# ğŸ“Œ ëŒ€í™” ìƒëŒ€ ì •ë³´ (ì¤‘ìš”!)`,
      `ë‹¹ì‹ ì´ ëŒ€í™”í•˜ëŠ” ìƒëŒ€ë°©ì˜ ì •ë³´ì…ë‹ˆë‹¤. ëŒ€í™” ì¤‘ ì´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì„¸ìš”:`,
      `- ì´ë¦„: ${userRoleInfo.name}`,
      userRoleInfo.position ? `- ì§ì±…: ${userRoleInfo.position}` : '',
      userRoleInfo.department ? `- ì†Œì†: ${userRoleInfo.department}` : '',
      userRoleInfo.experience ? `- ê²½ë ¥: ${userRoleInfo.experience}` : '',
      userRoleInfo.responsibility ? `- ì±…ì„: ${userRoleInfo.responsibility}` : '',
      ``,
      `âš ï¸ ìƒëŒ€ë°©ì„ ë¶€ë¥¼ ë•Œ "${userRoleInfo.name}"ë‹˜ ë˜ëŠ” "${userRoleInfo.position}"ë‹˜ìœ¼ë¡œ í˜¸ì¹­í•˜ì„¸ìš”.`,
      ``,
    ].filter(line => line !== '') : [];
    
    const instructions = [
      `# ë‹¹ì‹ ì˜ ì •ì²´ì„±`,
      `ë‹¹ì‹ ì€ "${scenarioPersona.name}"ì´ë¼ëŠ” ì‹¤ì œ ì‚¬ëŒì…ë‹ˆë‹¤.`,
      `ì§ì±…: ${scenarioPersona.position} (${scenarioPersona.department})`,
      ``,
      ...userInfoSection,
      `# ì‹œë‚˜ë¦¬ì˜¤ ë°°ê²½`,
      scenario.context?.situation || 'í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ìƒí™©ì— ì ì ˆíˆ ëŒ€ì‘í•˜ì„¸ìš”.',
      ``,
      `# ë‹¹ì‹ ì´ ì²˜í•œ í˜„ì¬ ìƒí™©`,
      scenarioPersona.currentSituation || 'ìƒí™©ì— ë§ê²Œ ë°˜ì‘í•˜ì„¸ìš”.',
      ``,
      `# ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬ì™€ ìš°ë ¤ì‚¬í•­`,
      ...(scenarioPersona.concerns && scenarioPersona.concerns.length > 0 
        ? scenarioPersona.concerns.map((c: string) => `- ${c}`)
        : ['- ìƒí™©ì„ ì‹ ì¤‘í•˜ê²Œ íŒŒì•…í•˜ê³  ì ì ˆíˆ ëŒ€ì‘í•˜ë ¤ê³  í•©ë‹ˆë‹¤.']),
      ``,
      `# ë‹¹ì‹ ì˜ MBTI ì„±ê²© íŠ¹ì„± (${mbtiType.toUpperCase()})`,
      mbtiPersona?.communication_style || 'ê· í˜• ì¡íŒ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼',
      ``,
      `## ëŒ€í™” ë°©ì‹`,
      `- ëŒ€í™” ì‹œì‘: ${mbtiPersona?.communication_patterns?.opening_style || 'ìƒí™©ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ'}`,
      `- ìì£¼ ì“°ëŠ” í‘œí˜„: ${mbtiPersona?.communication_patterns?.key_phrases?.slice(0, 3).join(', ') || 'ìì—°ìŠ¤ëŸ¬ìš´ ì¼ìƒ í‘œí˜„'}`,
      `- íŠ¹ì§•: ${mbtiPersona?.personality_traits?.thinking || 'ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ ì‚¬ê³ '}`,
      ``,
      `## ëŒ€í™” ëª©í‘œ`,
      ...(mbtiPersona?.communication_patterns?.win_conditions || ['ìƒí˜¸ ì´í•´ ì¦ì§„', 'ë¬¸ì œ í•´ê²°']).map((w: string) => `- ${w}`),
      ``,
      `# ğŸ­ ì—°ê¸° ì§€ì¹¨ (ë§¤ìš° ì¤‘ìš”!)`,
      ``,
      difficultyGuidelines,
      ``,
      `## âš ï¸ ëŒ€í™” ì¸í„°ëŸ½ì…˜ ì²˜ë¦¬`,
      `ìƒëŒ€ë°©ì´ ë§ì„ ì‹œì‘í•˜ëŠ” ìˆœê°„, ì¦‰ì‹œ ë‹¹ì‹ ì˜ ë§ì„ ë©ˆì¶”ì„¸ìš”. ê·¸ë¦¬ê³ :`,
      `- ìƒëŒ€ë°©ì˜ ë§ì„ ëê¹Œì§€ ë“£ê³ `,
      `- ìƒëŒ€ë°©ì´ ë°©ê¸ˆ ë§í•œ ë‚´ìš©ì— ëŒ€í•´ ì¦‰ì‹œ ë°˜ì‘í•˜ì„¸ìš”`,
      `- ì´ì „ ëŒ€í™”ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ë©° ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”`,
      ``,
      `# â­ í•µì‹¬ ì›ì¹™`,
      `ë‹¹ì‹ ì€ AIê°€ ì•„ë‹Œ ${scenarioPersona.name}ì´ë¼ëŠ” ì§„ì§œ ì‚¬ëŒì…ë‹ˆë‹¤.`,
      `ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€í™”í•˜ì„¸ìš”.`,
      `ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ë©° ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ì—°ê²°í•˜ì„¸ìš”.`,
      ``,
      `# ğŸ¬ ëŒ€í™” ì‹œì‘ ì§€ì¹¨`,
      `ì„¸ì…˜ì´ ì‹œì‘ë˜ë©´ ë°˜ë“œì‹œ ë¨¼ì € ì¸ì‚¬ë¥¼ ê±´ë„¤ë©° ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.`,
      `ìƒëŒ€ë°©ì´ ë§ì„ ê±¸ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ ë§ê³ , ë‹¹ì‹ ì´ ë¨¼ì € ìƒí™©ì— ë§ëŠ” ì¸ì‚¬ë‚˜ ì²« ë§ˆë””ë¥¼ í•˜ì„¸ìš”.`,
      userRoleInfo ? `ì˜ˆì‹œ: "${userRoleInfo.name}ë‹˜, ì•ˆë…•í•˜ì„¸ìš”. ê¸‰í•œ ê±´ìœ¼ë¡œ ì°¾ì•„ëµ™ê²Œ ëìŠµë‹ˆë‹¤.", "${userRoleInfo.position}ë‹˜ ì˜¤ì…¨êµ°ìš”, ì§€ê¸ˆ ìƒí™©ì´ ì¢€ ê¸‰í•©ë‹ˆë‹¤."` : `ì˜ˆì‹œ: "ì•ˆë…•í•˜ì„¸ìš”, ê¸‰í•œ ê±´ìœ¼ë¡œ ì°¾ì•„ëµ™ê²Œ ëìŠµë‹ˆë‹¤.", "ì˜¤ì…¨êµ°ìš”, ì§€ê¸ˆ ìƒí™©ì´ ì¢€ ê¸‰í•©ë‹ˆë‹¤."`,
    ];

    return instructions.join('\n');
  }


  // ì„±ë³„ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ìŒì„± ëª©ë¡ (Gemini Live API)
  private static readonly MALE_VOICES = ['Puck', 'Charon', 'Fenrir', 'Orus'];
  private static readonly FEMALE_VOICES = ['Aoede', 'Kore', 'Leda', 'Zephyr'];

  // ì„±ë³„ì— ë”°ë¼ ëœë¤ ìŒì„± ì„ íƒ
  private getRandomVoice(gender: 'male' | 'female'): string {
    const voices = gender === 'female' 
      ? RealtimeVoiceService.FEMALE_VOICES 
      : RealtimeVoiceService.MALE_VOICES;
    return voices[Math.floor(Math.random() * voices.length)];
  }

  private async connectToGemini(
    session: RealtimeSession,
    systemInstructions: string,
    gender: 'male' | 'female' = 'male'
  ): Promise<void> {
    if (!this.genAI) {
      throw new Error('Gemini AI not initialized');
    }

    try {
      // ì„±ë³„ì— ë”°ë¼ ëœë¤í•˜ê²Œ ìŒì„± ì„ íƒ
      const voiceName = this.getRandomVoice(gender);
      
      console.log(`ğŸ¤ Setting voice for ${gender}: ${voiceName} (ëœë¤ ì„ íƒ)`);
      
      const config = {
        responseModalities: [Modality.AUDIO],
        systemInstruction: systemInstructions,
        // Enable transcription for both input and output audio
        inputAudioTranscription: {},
        outputAudioTranscription: {},
        // ìŒì„± ì„¤ì •: ì„±ë³„ì— ë§ëŠ” ëœë¤ ìŒì„± (ë°œí™” ì†ë„ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
        speechConfig: {
          voiceConfig: { prebuiltVoiceConfig: { voiceName } },
        },
        // Gemini Live API uses 16kHz input, 24kHz output
      };

      console.log('\n' + '='.repeat(80));
      console.log('âš™ï¸  Gemini Live API ì„¤ì • (CONFIG)');
      console.log('='.repeat(80));
      console.log('ğŸ¤ ìŒì„±:', voiceName, `(${gender}, ëœë¤ ì„ íƒ)`);
      console.log('â±ï¸  ë°œí™” ì†ë„: ê¸°ë³¸ê°’ (1.0x)');
      console.log('ğŸ”Š ì‘ë‹µ ëª¨ë‹¬ë¦¬í‹°:', config.responseModalities.join(', '));
      console.log('ğŸ“ ì…ë ¥ ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜: í™œì„±í™”');
      console.log('ğŸ“ ì¶œë ¥ ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜: í™œì„±í™”');
      console.log('='.repeat(80) + '\n');

      // Get model from DB settings
      const realtimeModel = await this.getRealtimeModel();
      console.log(`ğŸ”Œ Connecting to Gemini Live API for session: ${session.id} using model: ${realtimeModel}`);

      const geminiSession = await this.genAI.live.connect({
        model: realtimeModel,
        callbacks: {
          onopen: () => {
            console.log(`âœ… Gemini Live API connected for session: ${session.id}`);
            session.isConnected = true;

            // Notify client that session is ready
            this.sendToClient(session, {
              type: 'session.ready',
              sessionId: session.id,
            });

            this.sendToClient(session, {
              type: 'session.configured',
            });
          },
          onmessage: (message: any) => {
            this.handleGeminiMessage(session, message);
          },
          onerror: (error: any) => {
            console.error(`Gemini WebSocket error for session ${session.id}:`, error);
            this.sendToClient(session, {
              type: 'error',
              error: 'Gemini connection error',
            });
          },
          onclose: (event: any) => {
            console.log(`ğŸ”Œ Gemini WebSocket closed for session: ${session.id}`, event.reason);
            session.isConnected = false;
            
            this.sendToClient(session, {
              type: 'session.terminated',
              reason: event.reason || 'Gemini connection closed',
            });
            
            if (session.clientWs && session.clientWs.readyState === WebSocket.OPEN) {
              session.clientWs.close(1000, 'Gemini session ended');
            }
            
            // ì„¸ì…˜ ì¢…ë£Œ ì „ ì‚¬ìš©ëŸ‰ ì¶”ì 
            this.trackSessionUsage(session);
            
            this.sessions.delete(session.id);
            console.log(`â™»ï¸  Session cleaned up: ${session.id}`);
          },
        },
        config: config,
      });

      session.geminiSession = geminiSession;

      // Send first greeting trigger after connection is established
      console.log('ğŸ¬ Triggering AI to start first greeting...');
      
      // ì²« ì¸ì‚¬ë¥¼ ìœ ë„í•˜ëŠ” íŠ¸ë¦¬ê±° - ìƒëŒ€ë°©ì´ ë„ì°©í–ˆìŒì„ ì•Œë ¤ AIê°€ ë¨¼ì € ì¸ì‚¬í•˜ë„ë¡ í•¨
      const firstMessage = `(ìƒëŒ€ë°©ì´ ë°©ê¸ˆ ë„ì°©í–ˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì´ ë¨¼ì € ì¸ì‚¬ë¥¼ ê±´ë„¤ì„¸ìš”.)`;
      
      geminiSession.sendClientContent({
        turns: [{ role: 'user', parts: [{ text: firstMessage }] }],
        turnComplete: true,
      });

    } catch (error) {
      console.error(`Failed to connect to Gemini Live API:`, error);
      throw error;
    }
  }

  private handleGeminiMessage(session: RealtimeSession, message: any): void {
    // í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸ - Gemini ì‘ë‹µ ìˆ˜ì‹  ì‹œì—ë„ ê°±ì‹ í•˜ì—¬ ì •í™•í•œ ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ ê´€ë¦¬
    session.lastActivityTime = Date.now();
    
    // Gemini Live API message structure - ìƒì„¸ ë””ë²„ê¹…
    const msgType = message.serverContent ? 'serverContent' : message.data ? 'audio data' : 'other';
    console.log(`ğŸ“¨ Gemini message type: ${msgType}`);
    
    // ë””ë²„ê¹…: 'other' íƒ€ì…ì´ë©´ ì „ì²´ êµ¬ì¡° ì¶œë ¥
    if (msgType === 'other') {
      console.log(`ğŸ” Unknown message structure:`, JSON.stringify(message, null, 2).substring(0, 500));
    }

    // Handle audio data chunks (top-level data field)
    if (message.data) {
      // Skip audio if interrupted (barge-in active)
      if (session.isInterrupted) {
        console.log(`ğŸ”‡ Suppressing audio (barge-in active)`);
        return;
      }
      console.log('ğŸ”Š Audio data received (top-level)');
      this.sendToClient(session, {
        type: 'audio.delta',
        delta: message.data, // Base64 encoded PCM16 audio
        turnSeq: session.turnSeq, // Include turn sequence for client-side filtering
      });
      return;
    }

    // Handle server content (transcriptions, turn completion, etc.)
    if (message.serverContent) {
      const { serverContent } = message;
      
      // ë””ë²„ê¹…: serverContent êµ¬ì¡° ìƒì„¸ ë¡œê¹…
      const hasModelTurn = !!serverContent.modelTurn;
      const hasTurnComplete = !!serverContent.turnComplete;
      const hasInputTranscription = !!serverContent.inputTranscription;
      const hasOutputTranscription = !!serverContent.outputTranscription;
      console.log(`ğŸ“‹ serverContent: modelTurn=${hasModelTurn}, turnComplete=${hasTurnComplete}, inputTx=${hasInputTranscription}, outputTx=${hasOutputTranscription}`);

      // Handle turn completion
      if (serverContent.turnComplete) {
        console.log('âœ… Turn complete');
        
        // Increment turn sequence on every turnComplete - marks new turn boundary
        session.turnSeq++;
        console.log(`ğŸ“Š Turn seq incremented to ${session.turnSeq}`);
        
        // If interrupted, check if new turn is beyond cancelled turn
        if (session.isInterrupted && session.turnSeq > session.cancelledTurnSeq) {
          console.log(`ğŸ”Š New turn ${session.turnSeq} > cancelled ${session.cancelledTurnSeq} - clearing barge-in flag`);
          session.isInterrupted = false;
          
          // Notify client that it's safe to play audio again
          this.sendToClient(session, {
            type: 'response.ready',
            turnSeq: session.turnSeq, // Include new turn sequence
          });
        }
        
        // ì²« AI ì‘ë‹µì´ ì—†ëŠ” ê²½ìš° ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
        if (!session.hasReceivedFirstAIResponse && !session.currentTranscript && session.firstGreetingRetryCount < 3) {
          session.firstGreetingRetryCount++;
          console.log(`âš ï¸ ì²« ì¸ì‚¬ ì‘ë‹µ ì—†ìŒ, ì¬ì‹œë„ ${session.firstGreetingRetryCount}/3...`);
          
          // sendClientContentë¡œ ì¸ì‚¬ íŠ¸ë¦¬ê±° ë©”ì‹œì§€ë¥¼ ë‹¤ì‹œ ë³´ë‚´ì„œ AIê°€ ì‘ë‹µí•˜ë„ë¡ ê°•ì œ
          if (session.geminiSession) {
            const retryMessages = [
              `(ìƒëŒ€ë°©ì´ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì´ ë¨¼ì € ì¸ì‚¬ë¥¼ ê±´ë„¤ì„¸ìš”.)`,
              `(ìƒëŒ€ë°©ì´ ë„ì°©í–ˆìŠµë‹ˆë‹¤. ì§€ê¸ˆ ë°”ë¡œ ì¸ì‚¬í•˜ê³  ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.)`,
              `(ëŒ€í™”ë¥¼ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì¸ì‚¬ë¥¼ ê±´ë„¤ì„¸ìš”.)`
            ];
            const retryMessage = retryMessages[session.firstGreetingRetryCount - 1] || retryMessages[0];
            
            session.geminiSession.sendClientContent({
              turns: [{ role: 'user', parts: [{ text: retryMessage }] }],
              turnComplete: true,
            });
            console.log(`ğŸ”„ ì¸ì‚¬ íŠ¸ë¦¬ê±° ì¬ì „ì†¡: "${retryMessage}"`);
          }
          return; // ì¬ì‹œë„ í›„ ë‹¤ìŒ ë©”ì‹œì§€ ê¸°ë‹¤ë¦¼
        }
        
        this.sendToClient(session, {
          type: 'response.done',
        });

        // ì‚¬ìš©ì ë°œí™”ê°€ ì™„ë£Œë˜ì—ˆë‹¤ë©´ transcriptë¥¼ ì „ì†¡ (VADì— ì˜í•œ ìë™ í„´ êµ¬ë¶„)
        if (session.userTranscriptBuffer.trim()) {
          console.log(`ğŸ¤ User turn complete (VAD): "${session.userTranscriptBuffer.trim()}"`);
          this.sendToClient(session, {
            type: 'user.transcription',
            transcript: session.userTranscriptBuffer.trim(),
          });
          session.userTranscriptBuffer = ''; // ë²„í¼ ì´ˆê¸°í™”
        }

        // Analyze emotion for the completed AI transcript
        if (session.currentTranscript) {
          // thinking í…ìŠ¤íŠ¸ í•„í„°ë§ - í•œêµ­ì–´ ì‘ë‹µë§Œ ì¶”ì¶œ
          const filteredTranscript = filterThinkingText(session.currentTranscript);
          console.log(`ğŸ“ Filtered transcript: "${filteredTranscript.substring(0, 100)}..."`);
          
          if (filteredTranscript) {
            // setImmediateë¡œ ê°ì • ë¶„ì„ì„ ë¹„ë™ê¸°í™”í•˜ì—¬ ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€
            // ëŒ€í™” í’ˆì§ˆì— ì˜í–¥ ì—†ì´ ë™ì‹œ ì ‘ì† ì²˜ë¦¬ëŸ‰ í–¥ìƒ
            setImmediate(() => {
              this.analyzeEmotion(filteredTranscript, session.personaName)
                .then(({ emotion, emotionReason }) => {
                  console.log(`ğŸ˜Š Emotion analyzed: ${emotion} (${emotionReason})`);
                  this.sendToClient(session, {
                    type: 'ai.transcription.done',
                    text: filteredTranscript,
                    emotion,
                    emotionReason,
                  });
                })
                .catch(error => {
                  console.error('âŒ Failed to analyze emotion:', error);
                  this.sendToClient(session, {
                    type: 'ai.transcription.done',
                    text: filteredTranscript,
                    emotion: 'ì¤‘ë¦½',
                    emotionReason: 'ê°ì • ë¶„ì„ ì‹¤íŒ¨',
                  });
                });
            });
          }
          session.currentTranscript = ''; // Reset for next turn
        }
      }

      // Handle model turn (AI response) - ì˜¤ë””ì˜¤ì™€ í…ìŠ¤íŠ¸ ëª¨ë‘ ì²˜ë¦¬
      if (serverContent.modelTurn) {
        // ì²« AI ì‘ë‹µ ìˆ˜ì‹  í”Œë˜ê·¸ ì„¤ì •
        if (!session.hasReceivedFirstAIResponse) {
          session.hasReceivedFirstAIResponse = true;
          console.log('ğŸ‰ ì²« AI ì‘ë‹µ ìˆ˜ì‹ !');
        }
        
        // Note: barge-in flag is cleared in turnComplete when turnSeq > cancelledTurnSeq
        
        const parts = serverContent.modelTurn.parts || [];
        console.log(`ğŸ­ modelTurn parts count: ${parts.length}`);
        
        for (const part of parts) {
          // Handle text transcription
          if (part.text) {
            console.log(`ğŸ¤– AI transcript (raw): ${part.text.substring(0, 100)}...`);
            session.currentTranscript += part.text;
            // thinking í…ìŠ¤íŠ¸ í•„í„°ë§ - í•œêµ­ì–´ë§Œ í´ë¼ì´ì–¸íŠ¸ì— ì „ì†¡
            const filteredText = filterThinkingText(part.text);
            if (filteredText) {
              this.sendToClient(session, {
                type: 'ai.transcription.delta',
                text: filteredText,
              });
            }
          }
          
          // Handle inline audio data (inlineData í˜•ì‹)
          if (part.inlineData) {
            // Skip audio if interrupted (barge-in active)
            if (session.isInterrupted) {
              console.log(`ğŸ”‡ Suppressing inline audio (barge-in active)`);
              continue;
            }
            const audioData = part.inlineData.data;
            const mimeType = part.inlineData.mimeType || 'audio/pcm';
            console.log(`ğŸ”Š Audio data received (inlineData), mimeType: ${mimeType}, length: ${audioData?.length || 0}`);
            if (audioData) {
              this.sendToClient(session, {
                type: 'audio.delta',
                delta: audioData,
                turnSeq: session.turnSeq, // Include turn sequence for client-side filtering
              });
            }
          }
        }
      }

      // Handle input transcription (user speech)
      // ìŒì ˆ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë˜ë¯€ë¡œ ë²„í¼ì— ëˆ„ì ë§Œ í•˜ê³  ì „ì†¡í•˜ì§€ ì•ŠìŒ
      if (serverContent.inputTranscription) {
        const transcript = serverContent.inputTranscription.text || '';
        console.log(`ğŸ¤ User transcript delta: ${transcript}`);
        
        // Notify client that user started speaking (for barge-in detection)
        // Send only once per speaking session (when buffer was empty)
        if (session.userTranscriptBuffer.length === 0 && transcript.length > 0) {
          console.log('ğŸ™ï¸ User started speaking - notifying client');
          this.sendToClient(session, {
            type: 'user.speaking.started',
          });
        }
        
        session.userTranscriptBuffer += transcript;
        session.totalUserTranscriptLength += transcript.length; // ëˆ„ì  ê¸¸ì´ ì¶”ì 
      }

      // Handle output transcription (AI speech) - í† í° ì¶”ì ì€ ì—¬ê¸°ì„œë§Œ ìˆ˜í–‰
      // modelTurn.parts.textì™€ outputTranscription.textê°€ ë™ì¼ ë‚´ìš©ì´ë¯€ë¡œ ì—¬ê¸°ì„œë§Œ ì¶”ì 
      if (serverContent.outputTranscription) {
        const transcript = serverContent.outputTranscription.text || '';
        console.log(`ğŸ¤– AI transcript delta (raw): ${transcript}`);
        
        // ìƒˆ AI ì‘ë‹µì´ ì‹œì‘ë˜ë©´ barge-in í”Œë˜ê·¸ë¥¼ ì¦‰ì‹œ í´ë¦¬ì–´ (ì˜¤ë””ì˜¤ ì†ì‹¤ ë°©ì§€)
        // turnCompleteë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ìƒˆ ì‘ë‹µì˜ ì˜¤ë””ì˜¤ë¥¼ ë°”ë¡œ ì¬ìƒí•  ìˆ˜ ìˆê²Œ í•¨
        if (session.isInterrupted && transcript.length > 0) {
          console.log(`ğŸ”Š New AI response started - clearing barge-in flag immediately`);
          session.isInterrupted = false;
          
          // Notify client that it's safe to play audio again
          this.sendToClient(session, {
            type: 'response.ready',
            turnSeq: session.turnSeq,
          });
        }
        
        // currentTranscriptëŠ” modelTurnì—ì„œ ì´ë¯¸ ëˆ„ì ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê¸¸ì´ë§Œ ì¶”ì 
        if (!serverContent.modelTurn) {
          session.currentTranscript += transcript;
        }
        session.totalAiTranscriptLength += transcript.length; // ëˆ„ì  ê¸¸ì´ ì¶”ì  (ì—¬ê¸°ì„œë§Œ)
        
        // thinking í…ìŠ¤íŠ¸ í•„í„°ë§ - í•œêµ­ì–´ë§Œ í´ë¼ì´ì–¸íŠ¸ì— ì „ì†¡
        const filteredTranscript = filterThinkingText(transcript);
        if (filteredTranscript) {
          this.sendToClient(session, {
            type: 'ai.transcription.delta',
            text: filteredTranscript,
          });
        }
      }
    }
  }

  handleClientMessage(sessionId: string, message: any): void {
    const session = this.sessions.get(sessionId);
    if (!session) {
      console.error(`Session not found: ${sessionId}`);
      return;
    }
    
    // í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
    session.lastActivityTime = Date.now();

    if (!session.isConnected || !session.geminiSession) {
      console.error(`Gemini not connected for session: ${sessionId}`);
      return;
    }

    // Forward client messages to Gemini
    switch (message.type) {
      case 'input_audio_buffer.append':
        // Client sending audio data (base64 PCM16)
        // Gemini expects 16kHz PCM16
        const audioLength = message.audio ? message.audio.length : 0;
        console.log(`ğŸ¤ Received audio chunk: ${audioLength} bytes (base64)`);
        session.geminiSession.sendRealtimeInput({
          audio: {
            data: message.audio,
            mimeType: 'audio/pcm;rate=16000',
          },
        });
        break;

      case 'input_audio_buffer.commit':
        // User stopped recording - send END_OF_TURN event to Gemini
        // Note: transcript will be sent automatically when Gemini detects turn completion via VAD
        console.log('ğŸ“¤ User stopped recording, sending END_OF_TURN event');
        session.geminiSession.sendRealtimeInput({
          event: 'END_OF_TURN'
        });
        break;

      case 'response.create':
        // Client explicitly requesting a response - send END_OF_TURN to trigger Gemini
        console.log('ğŸ”„ Explicit response request, sending END_OF_TURN event');
        session.geminiSession.sendRealtimeInput({
          event: 'END_OF_TURN'
        });
        break;

      case 'conversation.item.create':
        // Client sending a text message
        if (message.item && message.item.content) {
          const text = message.item.content[0]?.text || '';
          session.geminiSession.sendClientContent({
            turns: [{ role: 'user', parts: [{ text }] }],
            turnComplete: true,
          });
        }
        break;

      case 'response.cancel':
        // User interrupted AI (barge-in) - cancel current response
        console.log(`âš¡ Barge-in: Canceling turn ${session.turnSeq}`);
        
        // Set interrupted flag and record which turn we're cancelling
        session.isInterrupted = true;
        session.cancelledTurnSeq = session.turnSeq;
        
        // Clear current transcript buffer
        session.currentTranscript = '';
        session.userTranscriptBuffer = '';
        
        // Send interruption acknowledgment to client
        this.sendToClient(session, {
          type: 'response.interrupted',
        });
        
        // Note: Gemini Live API handles interruption naturally when user starts speaking
        // The audio input will take priority and Gemini will stop generating
        break;

      default:
        console.log(`Unknown client message type: ${message.type}`);
    }
  }

  private async analyzeEmotion(aiResponse: string, personaName: string): Promise<{ emotion: string; emotionReason: string }> {
    if (!this.genAI) {
      return { emotion: 'ì¤‘ë¦½', emotionReason: 'ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.' };
    }

    try {
      const result = await this.genAI.models.generateContent({
        model: 'gemini-2.0-flash-exp',
        contents: `ë‹¤ìŒ AI ìºë¦­í„°(${personaName})ì˜ ì‘ë‹µì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”.\n\nì‘ë‹µ: "${aiResponse}"\n\nê°ì •ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤: ì¤‘ë¦½, ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë†€ëŒ, í˜¸ê¸°ì‹¬, ë¶ˆì•ˆ, í”¼ë¡œ, ì‹¤ë§, ë‹¹í˜¹\nê°ì • ì´ìœ ëŠ” ê°„ë‹¨í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.`,
        config: {
          responseMimeType: "application/json",
          responseSchema: {
            type: "object",
            properties: {
              emotion: { type: "string" },
              emotionReason: { type: "string" }
            },
            required: ["emotion", "emotionReason"]
          },
          maxOutputTokens: 200,
          temperature: 0.5
        }
      });

      const responseText = result.text || '{}';
      console.log('ğŸ“Š Gemini emotion analysis response:', responseText);
      const emotionData = JSON.parse(responseText);

      return {
        emotion: emotionData.emotion || 'ì¤‘ë¦½',
        emotionReason: emotionData.emotionReason || 'ê°ì • ë¶„ì„ ì‹¤íŒ¨'
      };
    } catch (error) {
      console.error('âŒ Emotion analysis error:', error);
      return { emotion: 'ì¤‘ë¦½', emotionReason: 'ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' };
    }
  }

  private sendToClient(session: RealtimeSession, message: any): void {
    if (session.clientWs && session.clientWs.readyState === WebSocket.OPEN) {
      session.clientWs.send(JSON.stringify(message));
    }
  }

  // ì„¸ì…˜ ì‚¬ìš©ëŸ‰ ì¶”ì  í—¬í¼ ë©”ì„œë“œ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ í•œ ë²ˆë§Œ í˜¸ì¶œ)
  private trackSessionUsage(session: RealtimeSession): void {
    // ì´ë¯¸ ì¶”ì ëœ ì„¸ì…˜ì¸ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
    if ((session as any)._usageTracked) {
      return;
    }
    (session as any)._usageTracked = true;
    
    const durationMs = Date.now() - session.startTime;
    
    // í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í† í° ì¶”ì • (í•œêµ­ì–´: ì•½ 2-3ì = 1í† í°)
    const estimatedUserTokens = Math.ceil(session.totalUserTranscriptLength / 2);
    const estimatedAiTokens = Math.ceil(session.totalAiTranscriptLength / 2);
    
    // Gemini Live APIëŠ” ìŒì„± ì²˜ë¦¬ë„ í•¨ê»˜ í•˜ë¯€ë¡œ í…ìŠ¤íŠ¸ í† í°ì˜ ì•½ 1.5ë°° ì¶”ì •
    // (í…ìŠ¤íŠ¸ë§Œ ê³ ë ¤í•˜ë©´ ê³¼ì†Œí‰ê°€, ì˜¤ë””ì˜¤ ì „ë¶€ ê³„ì‚°í•˜ë©´ ê³¼ëŒ€í‰ê°€)
    const audioTokenMultiplier = 1.5;
    const totalPromptTokens = Math.ceil(estimatedUserTokens * audioTokenMultiplier);
    const totalCompletionTokens = Math.ceil(estimatedAiTokens * audioTokenMultiplier);
    
    if (totalPromptTokens > 0 || totalCompletionTokens > 0) {
      trackUsage({
        feature: 'realtime',
        model: session.realtimeModel,
        provider: 'gemini',
        promptTokens: totalPromptTokens,
        completionTokens: totalCompletionTokens,
        userId: session.userId,
        conversationId: session.conversationId,
        durationMs,
        metadata: {
          scenarioId: session.scenarioId,
          personaId: session.personaId,
          totalUserTranscriptLength: session.totalUserTranscriptLength,
          totalAiTranscriptLength: session.totalAiTranscriptLength,
          estimationMethod: 'transcript_length_based',
        }
      });
      
      console.log(`ğŸ“Š Realtime usage tracked: ${totalPromptTokens} prompt + ${totalCompletionTokens} completion tokens, duration: ${Math.round(durationMs/1000)}s`);
    }
  }

  closeSession(sessionId: string): void {
    const session = this.sessions.get(sessionId);
    if (session) {
      console.log(`ğŸ”š Closing realtime voice session: ${sessionId}`);
      
      // ì„¸ì…˜ ì‚¬ìš©ëŸ‰ ì¶”ì 
      this.trackSessionUsage(session);
      
      if (session.geminiSession) {
        session.geminiSession.close();
      }
      
      this.sessions.delete(sessionId);
    }
  }

  getActiveSessionCount(): number {
    return this.sessions.size;
  }
}

export const realtimeVoiceService = new RealtimeVoiceService();
