import WebSocket from 'ws';
import OpenAI from 'openai';
import { fileManager } from './fileManager';
import { GoogleGenerativeAI } from '@google/genai';

// OpenAI Realtime API - using GA model
const REALTIME_MODEL = 'gpt-realtime';

interface RealtimeSession {
  id: string;
  conversationId: string;
  scenarioId: string;
  personaId: string;
  personaName: string; // Store persona name for first greeting
  userId: string;
  clientWs: WebSocket;
  openaiWs: WebSocket | null;
  isConnected: boolean;
  audioBuffer: Buffer[];
}

export class RealtimeVoiceService {
  private sessions: Map<string, RealtimeSession> = new Map();
  private openai: OpenAI | null = null;
  private genAI: GoogleGenerativeAI | null = null;
  private isAvailable: boolean = false;

  constructor() {
    console.log("[OPENAI] key:", process.env.OPENAI_API_KEY?.slice(0, 12));
    console.log("[OPENAI] org:", process.env.OPENAI_ORG);
    console.log("[OPENAI] project:", process.env.OPENAI_PROJECT);
    
    if (process.env.OPENAI_API_KEY) {
      this.openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
      this.isAvailable = true;
      console.log('âœ… OpenAI Realtime Voice Service initialized');
    } else {
      console.warn('âš ï¸  OPENAI_API_KEY not set - Realtime Voice features disabled');
    }

    // Initialize Gemini for emotion analysis
    if (process.env.GOOGLE_GEMINI_API_KEY) {
      this.genAI = new GoogleGenerativeAI(process.env.GOOGLE_GEMINI_API_KEY);
      console.log('âœ… Gemini API initialized for emotion analysis');
    } else {
      console.warn('âš ï¸  GOOGLE_GEMINI_API_KEY not set - Emotion analysis disabled');
    }
  }

  isServiceAvailable(): boolean {
    return this.isAvailable;
  }

  async createSession(
    sessionId: string,
    conversationId: string,
    scenarioId: string,
    personaId: string,
    userId: string,
    clientWs: WebSocket
  ): Promise<void> {
    if (!this.isAvailable || !this.openai) {
      throw new Error('OpenAI Realtime Voice Service is not available. Please configure OPENAI_API_KEY.');
    }

    console.log(`ğŸ™ï¸ Creating realtime voice session: ${sessionId}`);

    // Load scenario and persona data
    const scenarios = await fileManager.getAllScenarios();
    const scenarioObj = scenarios.find(s => s.id === scenarioId);
    if (!scenarioObj) {
      throw new Error(`Scenario not found: ${scenarioId}`);
    }

    const scenarioPersona: any = scenarioObj.personas.find((p: any) => p.id === personaId);
    if (!scenarioPersona) {
      throw new Error(`Persona not found: ${personaId}`);
    }

    // Load MBTI personality traits
    const mbtiType: string = scenarioPersona.personaRef?.replace('.json', '') || '';
    const mbtiPersona = mbtiType ? await fileManager.getPersonaByMBTI(mbtiType) : null;

    // Create system instructions combining scenario context and MBTI traits
    const systemInstructions = this.buildSystemInstructions(
      scenarioObj,
      scenarioPersona,
      mbtiPersona
    );

    // Create session object
    const session: RealtimeSession = {
      id: sessionId,
      conversationId,
      scenarioId,
      personaId,
      personaName: scenarioPersona.name, // Store persona name
      userId,
      clientWs,
      openaiWs: null,
      isConnected: false,
      audioBuffer: [],
    };

    this.sessions.set(sessionId, session);

    // Connect to OpenAI Realtime API
    await this.connectToOpenAI(session, systemInstructions);
  }

  private buildSystemInstructions(
    scenario: any,
    scenarioPersona: any,
    mbtiPersona: any
  ): string {
    const mbtiType = scenarioPersona.personaRef?.replace('.json', '') || 'UNKNOWN';
    
    const instructions = [
      `ë‹¹ì‹ ì€ "${scenarioPersona.name}"ì…ë‹ˆë‹¤.`,
      `ì—­í• : ${scenarioPersona.position} (${scenarioPersona.department})`,
      ``,
      `# ì‹œë‚˜ë¦¬ì˜¤ ë°°ê²½`,
      scenario.context?.situation || '',
      ``,
      `# í˜„ì¬ ìƒí™©`,
      scenarioPersona.currentSituation || '',
      ``,
      `# ë‹¹ì‹ ì˜ ì„±ê²© íŠ¹ì„± (MBTI: ${mbtiType})`,
      mbtiPersona?.communication_style || 'ê· í˜• ì¡íŒ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼',
      ``,
      `# ëŒ€í™” íŒ¨í„´`,
      `- ì‹œì‘ ìŠ¤íƒ€ì¼: ${mbtiPersona?.communication_patterns?.opening_style || 'ìƒí™©ì— ë§ê²Œ ëŒ€í™” ì‹œì‘'}`,
      `- ì£¼ìš” í‘œí˜„: ${mbtiPersona?.communication_patterns?.key_phrases?.slice(0, 3).join(', ') || ''}`,
      ``,
      `# ë‹¹ì‹ ì˜ ê´€ì‹¬ì‚¬ì™€ ìš°ë ¤ì‚¬í•­`,
      ...(scenarioPersona.concerns || []).map((c: string) => `- ${c}`),
      ``,
      `# ëŒ€í™” ëª©í‘œ`,
      ...(mbtiPersona?.communication_patterns?.win_conditions || []).map((w: string) => `- ${w}`),
      ``,
      `# ì¤‘ìš” ì§€ì‹œì‚¬í•­`,
      `- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ëŒ€í™”í•˜ì„¸ìš”`,
      `- ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± í†¤ê³¼ ì–µì–‘ì„ ì‚¬ìš©í•˜ì„¸ìš”`,
      `- ë‹¹ì‹ ì˜ ê°ì • ìƒíƒœë¥¼ ìŒì„±ì— ë°˜ì˜í•˜ì„¸ìš”`,
      `- ì§§ê³  ê°„ê²°í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš” (1-3ë¬¸ì¥)`,
      `- ì‚¬ìš©ìì˜ ë§ì„ ê²½ì²­í•˜ê³  ì ì ˆíˆ ë°˜ì‘í•˜ì„¸ìš”`,
    ];

    return instructions.join('\n');
  }

  private async connectToOpenAI(
    session: RealtimeSession,
    systemInstructions: string
  ): Promise<void> {
    const url = 'wss://api.openai.com/v1/realtime?model=' + REALTIME_MODEL;
    
    const openaiWs = new WebSocket(url, {
      headers: {
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
        'OpenAI-Beta': 'realtime=v1',
      },
    });

    session.openaiWs = openaiWs;

    openaiWs.on('open', () => {
      console.log(`âœ… OpenAI Realtime API connected for session: ${session.id}`);
      session.isConnected = true;

      // Configure session (API format - no type field needed)
      this.sendToOpenAI(session, {
        type: 'session.update',
        session: {
          model: REALTIME_MODEL,
          instructions: systemInstructions,
          voice: 'alloy',
          input_audio_transcription: {
            model: 'whisper-1', // Enable user speech transcription
          },
        },
      });

      // Notify client that session is ready
      this.sendToClient(session, {
        type: 'session.ready',
        sessionId: session.id,
      });
    });

    openaiWs.on('message', (data: WebSocket.Data) => {
      try {
        const event = JSON.parse(data.toString());
        this.handleOpenAIEvent(session, event);
      } catch (error) {
        console.error('Error parsing OpenAI message:', error);
      }
    });

    openaiWs.on('error', (error) => {
      console.error(`OpenAI WebSocket error for session ${session.id}:`, error);
      this.sendToClient(session, {
        type: 'error',
        error: 'OpenAI connection error',
      });
    });

    openaiWs.on('close', () => {
      console.log(`ğŸ”Œ OpenAI WebSocket closed for session: ${session.id}`);
      session.isConnected = false;
      
      // Notify client that OpenAI connection was closed
      this.sendToClient(session, {
        type: 'session.terminated',
        reason: 'OpenAI connection closed',
      });
      
      // Close client connection and clean up session
      if (session.clientWs && session.clientWs.readyState === WebSocket.OPEN) {
        session.clientWs.close(1000, 'OpenAI session ended');
      }
      
      this.sessions.delete(session.id);
      console.log(`â™»ï¸  Session cleaned up: ${session.id}`);
    });
  }

  private handleOpenAIEvent(session: RealtimeSession, event: any): void {
    console.log(`ğŸ“¨ OpenAI event: ${event.type}`);

    switch (event.type) {
      case 'session.created':
        this.sendToClient(session, {
          type: 'session.configured',
          ...event,
        });
        break;
      
      case 'session.updated':
        console.log('âœ… Session updated with our settings');
        console.log('ğŸ“‹ Updated session config:', JSON.stringify(event.session, null, 2));
        this.sendToClient(session, {
          type: 'session.configured',
          ...event,
        });
        // ì„¸ì…˜ì´ ì—…ë°ì´íŠ¸ë˜ë©´ AIê°€ ìë™ìœ¼ë¡œ ì²« ì¸ì‚¬ë¥¼ ì‹œì‘
        console.log('ğŸ¬ Triggering AI to start first greeting with full context...');
        
        // Create a contextual first message using stored persona name
        const firstMessage = `[ì‹œì‘] ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ë‹¹ì‹ ì€ ${session.personaName}ì…ë‹ˆë‹¤. ìƒí™©ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ì‚¬í•˜ê³  ëŒ€í™”ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ìŒì„±ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.`;
        
        console.log('ğŸ“ First message context:', firstMessage);
        
        // Add a conversation item first to prompt the AI
        this.sendToOpenAI(session, {
          type: 'conversation.item.create',
          item: {
            type: 'message',
            role: 'user',
            content: [
              {
                type: 'input_text',
                text: firstMessage,
              },
            ],
          },
        });
        
        // Then request audio response (GA API - no modalities parameter)
        this.sendToOpenAI(session, {
          type: 'response.create',
        });
        break;

      case 'conversation.item.input_audio_transcription.completed':
        console.log(`ğŸ¤ User said: ${event.transcript}`);
        this.sendToClient(session, {
          type: 'user.transcription',
          transcript: event.transcript,
        });
        break;

      case 'response.audio.delta':
        // Forward audio chunks to client
        console.log('ğŸ”Š Audio delta received');
        this.sendToClient(session, {
          type: 'audio.delta',
          delta: event.delta,
        });
        break;

      case 'response.output_audio.delta':
        // ì´ë¯¸ audio.deltaë¥¼ ë³´ë‚´ê³  ìˆë‹¤ë©´ ì´ê±´ ë¬´ì‹œ
        // console.log('ignore response.output_audio.delta');
        break;

      case 'response.audio_transcript.delta':
      case 'response.output_audio_transcript.delta':
        // Forward transcript to client (both event formats supported)
        console.log(`ğŸ¤– AI transcript: ${event.delta}`);
        this.sendToClient(session, {
          type: 'ai.transcription.delta',
          text: event.delta,  // âœ… text í•„ë“œ ì‚¬ìš© (delta ì•„ë‹˜)
        });
        break;

      case 'response.audio_transcript.done':
      case 'response.output_audio_transcript.done':
        // Complete transcript (both event formats supported)
        console.log(`âœ… AI full transcript: ${event.transcript}`);
        
        // ê°ì • ë¶„ì„ì„ ë¹„ë™ê¸°ë¡œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ ì „ì†¡
        this.analyzeEmotion(event.transcript, session.personaName)
          .then(({ emotion, emotionReason }) => {
            console.log(`ğŸ˜Š Emotion analyzed: ${emotion} (${emotionReason})`);
            this.sendToClient(session, {
              type: 'ai.transcription.done',
              text: event.transcript,
              emotion,
              emotionReason,
            });
          })
          .catch(error => {
            console.error('âŒ Failed to analyze emotion:', error);
            // ê°ì • ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì „ì†¡
            this.sendToClient(session, {
              type: 'ai.transcription.done',
              text: event.transcript,
              emotion: 'ì¤‘ë¦½',
              emotionReason: 'ê°ì • ë¶„ì„ ì‹¤íŒ¨',
            });
          });
        break;

      case 'response.done':
        console.log(`âœ… Response complete`);
        console.log(`ğŸ“Š Response details:`, JSON.stringify(event.response, null, 2));
        this.sendToClient(session, {
          type: 'response.done',
        });
        break;

      case 'error':
        console.error(`âŒ OpenAI error:`, event.error);
        // Don't close session on empty buffer errors (recoverable)
        if (event.error?.code === 'input_audio_buffer_commit_empty') {
          console.log('âš ï¸  Empty audio buffer - ignoring');
          return;
        }
        this.sendToClient(session, {
          type: 'error',
          error: event.error,
        });
        break;

      // Events to ignore (already handled or not needed by client)
      case 'conversation.item.created':
      case 'response.created':
      case 'response.output_item.added':
      case 'response.content_part.added':
      case 'response.content_part.done':
      case 'response.output_item.done':
      case 'response.audio.done':
      case 'response.output_audio.done':
      case 'rate_limits.updated':
      case 'input_audio_buffer.speech_started':
      case 'input_audio_buffer.speech_stopped':
      case 'input_audio_buffer.committed':
        // Silently ignore these events (already processed or not needed)
        break;

      default:
        // Log unknown events but don't forward (prevents duplicate audio)
        console.log(`ğŸ“¨ Unhandled OpenAI event: ${event.type}`);
        break;
    }
  }

  handleClientMessage(sessionId: string, message: any): void {
    const session = this.sessions.get(sessionId);
    if (!session) {
      console.error(`Session not found: ${sessionId}`);
      return;
    }

    if (!session.isConnected || !session.openaiWs) {
      console.error(`OpenAI not connected for session: ${sessionId}`);
      return;
    }

    // Forward client messages to OpenAI
    switch (message.type) {
      case 'input_audio_buffer.append':
        // Client sending audio data
        this.sendToOpenAI(session, {
          type: 'input_audio_buffer.append',
          audio: message.audio,
        });
        break;

      case 'input_audio_buffer.commit':
        // Client finished speaking
        this.sendToOpenAI(session, {
          type: 'input_audio_buffer.commit',
        });
        break;

      case 'response.create':
        // Client requesting a response
        this.sendToOpenAI(session, {
          type: 'response.create',
        });
        break;

      case 'conversation.item.create':
        // Client sending a text message
        this.sendToOpenAI(session, {
          type: 'conversation.item.create',
          item: message.item,
        });
        break;

      default:
        console.log(`Unknown client message type: ${message.type}`);
    }
  }

  private async analyzeEmotion(aiResponse: string, personaName: string): Promise<{ emotion: string; emotionReason: string }> {
    if (!this.genAI) {
      return { emotion: 'ì¤‘ë¦½', emotionReason: 'ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.' };
    }

    try {
      const model = this.genAI.models.generateContent({
        model: 'gemini-2.0-flash-exp',
        config: {
          responseMimeType: "application/json",
          responseSchema: {
            type: "object",
            properties: {
              emotion: { type: "string" },
              emotionReason: { type: "string" }
            },
            required: ["emotion", "emotionReason"]
          },
          maxOutputTokens: 200,
          temperature: 0.5
        },
        contents: [
          { 
            role: "user", 
            parts: [{ 
              text: `ë‹¤ìŒ AI ìºë¦­í„°(${personaName})ì˜ ì‘ë‹µì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”.\n\nì‘ë‹µ: "${aiResponse}"\n\nê°ì •ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤: ì¤‘ë¦½, ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë†€ëŒ\nê°ì • ì´ìœ ëŠ” ê°„ë‹¨í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.` 
            }] 
          }
        ],
      });

      const response = await model;
      const responseText = response.response?.candidates?.[0]?.content?.parts?.[0]?.text || '{}';
      const emotionData = JSON.parse(responseText);

      return {
        emotion: emotionData.emotion || 'ì¤‘ë¦½',
        emotionReason: emotionData.emotionReason || 'ê°ì • ë¶„ì„ ì‹¤íŒ¨'
      };
    } catch (error) {
      console.error('âŒ Emotion analysis error:', error);
      return { emotion: 'ì¤‘ë¦½', emotionReason: 'ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' };
    }
  }

  private sendToOpenAI(session: RealtimeSession, message: any): void {
    if (session.openaiWs && session.isConnected) {
      session.openaiWs.send(JSON.stringify(message));
    }
  }

  private sendToClient(session: RealtimeSession, message: any): void {
    if (session.clientWs && session.clientWs.readyState === WebSocket.OPEN) {
      session.clientWs.send(JSON.stringify(message));
    }
  }

  closeSession(sessionId: string): void {
    const session = this.sessions.get(sessionId);
    if (session) {
      console.log(`ğŸ”š Closing realtime voice session: ${sessionId}`);
      
      if (session.openaiWs) {
        session.openaiWs.close();
      }
      
      this.sessions.delete(sessionId);
    }
  }

  getActiveSessionCount(): number {
    return this.sessions.size;
  }
}

export const realtimeVoiceService = new RealtimeVoiceService();
